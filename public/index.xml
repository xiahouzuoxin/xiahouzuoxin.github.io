<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Monkeyzx</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Monkeyzx</description>
    <image>
      <title>Monkeyzx</title>
      <url>http://localhost:1313/papermod-cover.png</url>
      <link>http://localhost:1313/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 05 May 2024 20:23:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>机器学习建模——如何融入先验信息</title>
      <link>http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1%E5%A6%82%E4%BD%95%E8%9E%8D%E5%85%A5%E5%85%88%E9%AA%8C%E4%BF%A1%E6%81%AF/</link>
      <pubDate>Sun, 05 May 2024 20:23:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1%E5%A6%82%E4%BD%95%E8%9E%8D%E5%85%A5%E5%85%88%E9%AA%8C%E4%BF%A1%E6%81%AF/</guid>
      <description>背景 想要用ML模型拟合业务，但实际中经常会遇到类似下面的场景：
二手车价格预估：同一辆车，预估的价格与里程应该是严格负相关的； 保费预估：保额越高，保费越高； 理财产品中，比如余额宝，用户的利率敏感度预估（利率敏感度=用户余额变化/利率变化）：同一用户，理财产品利率越高，购买金额越高，余额越高； 如果直接用比如xgb或dnn模型拟合 X -&amp;gt; Y，
$$ p_y=f(x), where\ x = [x_c, x_p] $$ 其中f为xgb或dnn等具体模型，x_c为先验无关的特征（比如二手车里面的品牌和里程特征无关），x_p为先验相关的特征。由于噪声数据的存在以及模型中没有任何的相关性约束，模型最终的打分输出很难保证满足上面案例中的约束关系。
自然想到的方法 一种自然想到的方法是，将f拆解成2部分：
$$ p_y=f_c(x_c) + f_p(x_c,x_p) $$ 其中：
$f_c$主要建模先验无关特征，可以是任意复杂模型，其输入只能是$x_c$； $f_p$单独建模先验信息，必须是满足先验约束的模型，一般是满足单调性的模型，比如LR、IsotonicRegression等，其输入包含$x_p$； 根据这个思路，二手车价格的建模就可以表达为：predicted car_value = 基础平均价格（baseclean_trade） + 里程相关的价格（predicted mieage_adjustment）。
这种方法建模有个问题，数据集怎么构造？针对二手车预估这个问题，标签一般只有最终的车价，basecleantrade model和averagemileage模型的标签怎构造呢？
basecleantrade model：除里程相关特征（odometer，mileage等）外，其他特征group by计算平均价格作为标签，即同款车在不同里程下的平均价格作为标签； averagemileage：里程特征的平均作为标签，其他特征作为模型的输入； 从数据构造过程也可以看出，要采用这种方法的话，就一定得保证同款车有多个不同里程下的数据，这样效果才会好。
进一步——End2End 上面需要建模多个模型，是否能把多个模型融合做到End 2 End训练呢？答案是在DNN中加入先验信息。
同样对于二手车价格的建模，如下图，可以在DNN中针对里程这一项加入先验约束（mileageadjust module）。
图里画的是线性约束，当然，线性约束效果最中预估效果上可能会有一点折扣，可以考虑survival/cox function作为先验约束函数能达到更平滑的效果。具体用什么约束函数，是通过数据分析得到的。
根据这个思路，如果我要预估理财中的用户利率敏感度可以怎么做呢？把上面的思路顺一顺：
分析历史数据中“利率变化&amp;quot;和“余额变化&amp;quot;的关系，如果是接近线性，那就可以用线性先验，如果只是单调则可以考虑survival/cox function；也可以结合一些业务/经济规律来定义先验，比如需求价格弹性； DNN建模，在模型中加入步骤1中得到的先验约束； 最终，利率敏感度建模可以类似下面这样（先验有点类似于需求价格弹性先验）。其中$Q_t$是用户在t日的余额，$r_t$是产品在t日的利率，$Q_{t-&amp;gt;t+N}$用户余额变化，$x_{t^+}$是用户利率变化，模型训练完后的$E_d$即可输出作为利率敏感度。
扩展 其实了解过uplift因果模型的同学，应该很容易发现这里的建模方式和uplift中的s-learner很像，只是s-learner中一般treatment是二元变量，但是这里例子中的变量（比如利率，里程等）是连续型变量。
所以要继续深入可以研究将uplift中的方法引入到先验建模场景中。</description>
    </item>
    <item>
      <title>重读经典——word2vec</title>
      <link>http://localhost:1313/posts/%E9%87%8D%E8%AF%BB%E7%BB%8F%E5%85%B8word2vec/</link>
      <pubDate>Sun, 31 Mar 2024 15:32:32 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E9%87%8D%E8%AF%BB%E7%BB%8F%E5%85%B8word2vec/</guid>
      <description>为什么需要重读word2vec Word2vec原论文：
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781,2013. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. NIPS 2013. 其中1主要是描述CBOW和skip-gram相关方法，2描述了Negative Sampling以及训练word2vec的方法。为什么想要重读word2vec：
推荐系统中，可以把item或user类比与word2vec中的word，很多i2i和u2i的方法都是借鉴的word2vec，以及召回中常用的NCE Loss和Sampled Softmax Loss都和word2vec相关；熟悉word2vec对了解召回，对如何像Aribnb KDD18和阿里EGES一样将word2vec改进应用到具体业务中，会非常有帮助； 召回热门样本的处理，也可以借鉴word2vec中热门词的处理思路； GPT的输入词是字符串，怎么表征输入到NN？当然，现在大家都很熟悉了embedding lookup table，embedding lookup table的其实最早差不多就是来自word2vec（更早影响没那么大暂且不提）；沿着这个拓展，因为词表太大，又在embedding前加了tokenizer过程； word2vec是老祖级别的语言模型，那么最新的GPT和它主要区别在哪里？ 以及初次看GPT代码时，发现GPT的输出是softmax，word2vec其实loss也是softmax loss。但为什么在word2vec里面的softmax就需要Negative sampling，而GPT里却不需要？ CBOW和skip-gram word2vec核心思想是通过训练神经网络，将word映射到高维embedding。其embedding的思想对推荐系统、以及Bert,GPT等都影响深远。Word2vec一般有两种训练方法：CBOW和skip-gram。
CBOW是通过上下文（Context）学习当前word（BERT中的完形填空和这个很像），假设词与词之间相互独立，则对应的极大似然学习目标为：
$$ E=-\log p(w_t|w_{t-1},w_{t-2},..,w_{t-c},w_{t+1},w_{t+2},...,w_{t+c}) \\ = -\log \frac{e^{h \cdot v&#39;_o} }{\sum_{j \in V} e^{h \cdot v&#39;_j }} \\ = - h \cdot v&#39;_o + \log {\sum_{j \in V} e^{h \cdot v&#39;_j}} $$ 其中，</description>
    </item>
    <item>
      <title>超长行为序列建模SDIM</title>
      <link>http://localhost:1313/posts/%E8%B6%85%E9%95%BF%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1sdim/</link>
      <pubDate>Sun, 24 Mar 2024 15:01:46 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E8%B6%85%E9%95%BF%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1sdim/</guid>
      <description>SIM和ETA的问题 SIM通过类目从长序列中检索相关的items（hard-search）或者通过embedding inner product计算从长序列中检索最相似的topK个item；SIM的问题在 ETA 中讲过，主要是离线索引可能带来的在线target item embedding和离线的item embeddings不一致的问题； ETA在SIM基础上，通过对Long Behavior Sequence的item embeddings进行SimHash（LSH）之后，然后就可以将inner product的耗时计算转化为Hamming distance计算。从而大大降低了计算量，可以把检索topK的过程放到线上模型中，解决了SIM在离线不一致的问题；但是ETA依然需要通过一个Multi Head Target Attention得到最终的Long Behavior Sequence Embedding表达； 不管是SIM还是ETA，都是基于检索的方法从长序列（&amp;gt;1000）的候选items中选出最相关的topK，美团的这篇文章Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction 则是吸收了ETA中Hash函数的特点，但不使用检索的方法，不通过Multi Head Target Attention，直接得到用户Long Behavior Sequence的embedding表达——SDIM（Sampling-based Deep Interest Modeling）。
Hash-Based Sampling的用户行为建模 SimHash有Local Sentitive特性，也就是说空间距离相近的两个vector在Hash之后距离也 大概率 相近。SDIM就是利用这个原理：
将序列的所有Item Embeddings和Target Embedding通过m个Hash函数进行Hash（比如下图m=4） Item Embedding每次Hash之后和Target Embedding落在同一个桶（Hash的LSH特性），则记为一次贡献； 进行m次Hash之后，统计每个item embedding的贡献，最后用L2 normalization进行归一化得到每个item embedding的权重 weights（按理L1也可以，但是作者说L2效果更好） 最后只需要将长序列的item embeddings通过weighted sum pooling求和就得到了Long Behavior Sequence的Embedding表达，完美避开了Multi Head Target Attention的操作； 当然，为了降低Hash冲突的风险，实际中需要将hash function数量m和Hash width设置得大一些。</description>
    </item>
    <item>
      <title>超长行为序列建模ETA</title>
      <link>http://localhost:1313/posts/%E8%B6%85%E9%95%BF%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1eta/</link>
      <pubDate>Sat, 23 Mar 2024 16:04:10 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E8%B6%85%E9%95%BF%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1eta/</guid>
      <description>SIM的问题 前面介绍过阿里巴巴超长行为序列建模的方法SIM是two-stage，在预估的时候，SIM先通过target item从长行为序列中通过类目（hard-search）或item embedding相似度（soft-search）检索出top-K，再送到主模型中做target attention。由于类目索引和item embedding索引的构建是离线的，而CTR主模型很多时候需要online learning，这种情况下就造成用于检索的目标item embedding和离线构建的item embedding不在同一空间（离线的item索引存在滞后）。
End-to-End User Behavior Retrieval in Click-Through Rate Prediction Model 是阿里主搜团队 基于此出发提出了End-to-end Target Attention - ETA，GMV提升3%。我们回到长序列问题本身，为啥不能像DIN/DIEN一样直接对超长序列做target attention——因为inference time吃不消（target attention需要对长序列中的每个item emebedding做inner product）。那么有没有其它方法可以快速从长序列中检索出top-K，又不需要像SIM那样最终要用到比如faiss做快速的索引？
ETA就想到了SimHash！通过将long behavior sequence item embeddings进行SimHash后，就能通过Hamming distance替代掉inner product，从而缓解长序列中inner product所带来的计算量问题。
Embedding SimHash后能快速的进行TopK检索 SimHash算法用于将每个item embedding映射成一个二进制的串（最终可以保存成一个整数），之前主要用于长文本相似度的比较。这里应用SimHash相当于把embedding当作长文本。
本文使用的SimHash伪代码如下，代码中共进行m次Hash，Hash的width=2即每次Hash返回二值0或1的$sig_k[i]$：
只要满足下面的条件，就能保证最终的计算复杂度能够降低：
映射后的向量相似度计算函数的复杂度比inner product低（映射后的二进制向量用Hamming distance计算相似度，复杂度 &amp;lt; inner product）； 映射后的向量长度比原item embedding长度小（m&amp;lt;d）； Hash映射函数的复杂度不高；从伪代码看，Hash过程也需要计算内积，怎么感觉比inner product还高，Table 2里面的ETA的Retrieval complexity貌似也没把Hash函数的计算量考虑进去？主要原因是：在inference阶段，模型训练好之后，item embedding都是lookup table，那我们可以把lookup table里面的所有embeddings都提前计算好它的$sig_k$，然后和Embedding Table以及模型一起存储下来，最终只要根据item查表得到$sig_k$就行。所以Inference的时候，理论上Hash函数的时间可以忽略不计； 下图是采用SimHash之后，ETA对各种检索方法的时间复杂度、检索信息一致性的对比：相比直接inner product，ETA通过对item embedding做SimHash，然后用Hamming distance相似度函数进行检索。
有几个问题：
SimHash把d=128维度的embedding映射到了m=4维，且通过Hamming distance代替了inner product计算，这也就是ETA为什么能降低计算复杂度的核心原因。那如何保证映射后m=4维的向量后原d=128 embedding的信息不会丢失呢？SimHash有一个特殊的性质——locality sensitive hashing，即局部敏感哈希，也就是说空间上相近的两个embedding做完Hash之后位置依然大概率接近。 如果是SimHash映射到m=4然后再计算相似度，那为什么不建embedding lookup table的时候就让d=4？核心是因为这里的复杂度降低，是将inner product计算替换成了Hamming distance计算，Hamming distance通过异或XOR运算复杂度可以控制在O(1)，所以不管怎么降低embedding dim都比较难达到Hamming distance这么低的计算复杂度。 ETA (End-to-end Target Attention) ETA结构左边就是SimHash的应用，具体的：</description>
    </item>
    <item>
      <title>超长行为序列建模SIM</title>
      <link>http://localhost:1313/posts/%E8%B6%85%E9%95%BF%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1sim/</link>
      <pubDate>Thu, 21 Mar 2024 18:00:55 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E8%B6%85%E9%95%BF%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1sim/</guid>
      <description>为什么推荐需要超长行为序列？ 试想这么种场景，某个用户4个月前在某家店买了厕纸，2个月前又买了厕纸，然后中间又浏览了成千上万的其他东西，到现在又隔了两个月，应不应该推厕纸？
然而，对于模型来说，因为2个月前到现在中间浏览了成千上万的其他东西，而DIN, DIEN, MIMN只能建模1000以下的行为序列，所以早就把厕纸从行为用户历史行为序列中剔除出去了。
所以：
第一，通过超长行为序列可以捕获用户长周期里面周期性或规律性的购买行为 第二，短序列只能刻画用户短时的兴趣偏好（买了几次盲盒，但无法确定是否二次元用户），但通过超长序列可以刻画用户的长期偏好（比如，除了最近几次的盲盒还买了很多手办，二次元用户） 任何事情也都有利弊，即使计算量不是问题，但直接把用户所有的历史行为序列到模型中，除了会引入有用信息，同时也引入了许多的噪音。如何从用户长历史中剔除噪声，提取有用的信息？SIM在一定程度上就是在解决这个问题。
题外话：LLM里面的RGA，不是也为了解决这种问题吗？
SIM Paper: Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction
SIM是Two-stage Model， 包括GSU和ESU阶段。注意，主结构中依然保留了User short-time behaviors的输入，在短时序的基础上做增量，毕竟最近的行为序列才是最重要的。
General Search Unit (GSU) GSU，通过线性时间复杂度，用于从原始超长序列中检索出top-K相关的子行为。文中介绍了两种GSU的方式，soft-search和hard-search。两种搜索的差别主要在序列中的item和目标item的相似度的计算上，
$$ r_i = \left\{ \begin{aligned} sign (C_i=C_a) &amp; &amp; hard-search \\ (W_be_i) \cdot (W_ae_a)^T &amp; &amp; soft-search \end{aligned} \right. $$ 其中，$C_a$是目标item的category，$C_i$是序列中behavior item的category，即只从长序列中筛选同category的items；soft-search则是直接通过behavior item embedding和目标item embedding的内积（inner product）计算相似度，SIM使用的检索算法是ALSH（这也是召回里面常用的user向量检索item向量的方法）。
关于soft-search的item embedding：
不能直接使用short behavior sequence的embedding，因为不在一个表达空间，embedding差异会比较大；所以才有了上图中左边Soft Search Training辅助任务； Soft Search在原文中是和后面的ESU一起训练，得到behavior item embedding矩阵，线上serving的时候通过类似i2i召回，内积检索得到topK的item；但是，一起训练成本太高，behavior item的长期表达在短期内变化不大，这里是否可以定期离线训练？ soft-search training的技巧：当life long sequential behavior实在太长的时候（内存放不下），可以随机采样子行为序列，对最终item embedding的分布影响不大； hard-search或soft-search的topK结果送到后续的ESU。</description>
    </item>
    <item>
      <title>Rethinking DQN</title>
      <link>http://localhost:1313/posts/rethinking-dqn/</link>
      <pubDate>Fri, 22 Mar 2019 19:47:48 +0800</pubDate>
      <guid>http://localhost:1313/posts/rethinking-dqn/</guid>
      <description>原文来自自己很早前写的一篇公众号文章：Rethinking DQN
Playing Atari with Deep Reinforcement Learning 为什么增加Replay Memory有效？
一个样本可能被多次随机采样到，在多次权重更新中被使用，多次学习，局部策略学习更有效 直接学习连续的游戏帧样本，前后帧之间存在强的相关性，从memory中随机采样打破了这种相关性 Replay Memory相当于使用Replay Memory长度时间的behaviour分布，避免使用当前时刻behaviour出现震荡和局部最优 针对第3点，避免陷入局部最优和震荡的情况（比如陷入 往左-&amp;gt;update Q-&amp;gt;max Qaction-&amp;gt;往左 这种局部循环），Replay Memory需要使用off-policy的策略，所以选择Q Learning
on-policy: when learning on-policy the current parameters determine the next data sample that the parameters are trained on off-policy: current parameters are different to those used to generate the sample DQN中因为要积累batch，使用Experience Replay，所以是典型off-policy的 DQN输入预处理方法：
RGB -&amp;gt; GrayScale resize，然后切掉游戏背景的黑边，最后size从(210,160)变成(84.84) 使用最近的4帧作为输入，因此输入为4x84x84 关于Reward：
由于原文准备在不同的游戏上使用相同的架构和学习参数，所以把正reward都设成1，负reward都设成0，0 reward不变。用这种截断reward的方式，限制了error的范围，从而可以使用相同的学习率参数 上面这种截断reward的方式对性能有一定的影响 优化方法：
RMSProp with batchsize=32 e-greedy的阈值概率：前面100 0000(1 million)帧从1到0.</description>
    </item>
    <item>
      <title>毕业季读过的那些书</title>
      <link>http://localhost:1313/posts/%E6%AF%95%E4%B8%9A%E5%AD%A3%E8%AF%BB%E8%BF%87%E7%9A%84%E9%82%A3%E4%BA%9B%E4%B9%A6/</link>
      <pubDate>Sat, 02 Jan 2016 07:19:37 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E6%AF%95%E4%B8%9A%E5%AD%A3%E8%AF%BB%E8%BF%87%E7%9A%84%E9%82%A3%E4%BA%9B%E4%B9%A6/</guid>
      <description>这些年我一直提醒自己一件事情，千万不要自己感动自己。大部分人看似的努力，不过是愚蠢导致的。什么熬夜看书到天亮，连续几天只睡几小时，多久没放假了，如果这些东西也值得夸耀，那么富士康流水线上任何一个人都比你努力多了。人难免天生有自怜的情绪，唯有时刻保持清醒，才能看清真正的价值在哪里。
——于宙TEDx演讲：我们这一代人的困惑
平时总拿追求安逸平凡作为生活的目标，其实是给自己懒惰找的借口而已。为什么要读书，下面是在知乎里看到的答案，
我读过很多书，但后来大部分都被我忘记了，那阅读的意义是什么？
答：当我还是个孩子的时候，我吃过很多食物，现在已经记不起来吃过什么了。但可以肯定的是，它们中的一部分已经长成为我的骨头和肉。
2015年，因为经历，才深深体会了宫崎骏的一句话，
你在的城市，天下起了雨，很想问你有没有带伞，可我却忍住了，因为我怕你说没带，而我却无能为力。
为缓解这种忍耐的情绪，在下半年实习找工作之余，增加了一些读书量，如下图为2015年7月份以来的读书单。
其中有一些是极力推荐的。我是个东坡迷，相较于林语堂的《苏东坡传》，感觉李一冰的更客观更详细——大部分以史学材料、东坡诗词和东坡同时代的文人札记（或书信等）为基础。东坡那“如食之有蝇，吐之乃已”的性格，即使在现代也是“一肚子的不合时宜”，估计也会沦落到晓松老师之流。然而，东坡身兼儒释道的正派而又不失率真豁达，才气逼人却是晓松老师不能比的。
《目送》是台湾作家龙应台在老年时去回顾中年的感悟。在文中她写了生命中两件“小事”：儿子十六岁到美国当交换学生，在机场，她看着儿子通过护照检查、进入海关，背影倏地消失，没有回头；多年后，她父亲在医院的最后时光，她又看着轮椅上被护士推回房的父亲背影。通过这些事，她开始理解个人生命中最私密、最深埋、最不可言喻的“伤逝”和“舍”，并在文中写道：
我慢慢地、慢慢地了解到，所谓父女母子一场，只不过意味着，你和他的缘分就是今生今世不断地在目送他的背影渐行渐远。
虽还没到那个年纪，体悟不算太多，但也无法抗拒作者文字的朴素深沉。我记得这么一段描述，
「火车突然停了，」母亲说，「车顶上趴着一堆人，有一个女的说憋不住了，无论如何要上厕所，就爬下来，她的小孩儿还留在车顶上头，让人家帮她抱一下。没想到，她一下来，车就动了。」
小的时候，有一次和妈妈外出，也经历了同样的窘境。妈妈去上厕所还没回来，车就动了，那时候的我胆小，也不知道怎么和司机说，然后就一直害怕，“妈妈丢了，妈妈丢了…………”
在我们整个成长的过程里，谁，教过我们怎么去面对痛苦、挫折、失败？它不在我们的家庭教育里，它不在小学、中学、大学的教科书或课程里，它更不在我们的大众传播里。家庭教育、学校教育、社会教育只教我们如何去追求卓越，从砍樱桃的华盛顿、悬梁刺股的孙敬、苏秦到平地起楼的比尔?盖茨，都是成功的典范。即使是谈到失败，目的只是要你绝地反攻，再度追求出人头地，譬如越王句践的卧薪尝胆，洗雪耻辱，譬如哪个战败的国王看见蜘蛛如何结网，不屈不挠。 我们拚命地学习如何成功冲刺一百米，但是没有人教过我们：你跌倒时，怎么跌得有尊严；你的膝盖破得血肉模糊时，怎么清洗伤口、怎么包扎；你痛得无法忍受时，用什么样的表情去面对别人；你一头栽下时，怎么治疗内心淌血的创痛，怎么获得心灵深层的平静，心像玻璃一样碎了一地时，怎么收拾？
也成为一种教育悲哀的阐述——只教会如何坚持如何的努力，却没有告诉我们如何去缓解努力过程中带来的压力。
《小王子》里记得有那么一段话居然影响到我写简历，
如果你对大人们说：“我看到一幢玫瑰色的砖盖成的漂亮的房子，它的窗户上有天竺葵，屋顶上还有鸽子…”他们怎么也想象不出这种房子有多么好。必须对他们说：“我看见了一幢价值十万法郎的房子。”那么他们就会惊叫道：“多么漂亮的房子啊！”
那就是，写简历的时候能用数字说话就用数字说话，别BB“啊我那个项目做得多好啊”你还不如直接说“我做的那个项目拉了100万投资”。我本来是寻找童真的，却收获到一点利益世俗。但是，却也有很多地方感动到心跳加速，比如小王子到玫瑰盛开的花园，拿玫瑰花园里的花和自己星球上的玫瑰做对比，
“你们很美，但你们是空虚的。”小王子仍然在对她们说，“没有人能为你们去死。当然喽，我的那朵玫瑰花，一个普通的过路人以为她和你们一样。可是， 她单独一朵就比你们全体更重要，因为她是我浇灌的。因为她是我放在花罩中的。 因为她是我用屏风保护起来的。因为她身上的毛虫（除了留下两三只为了变蝴蝶 而外）是我除灭的。因为我倾听过她的怨艾和自诩，甚至有时我聆听着她的沉默。 因为她是我的玫瑰。”
“因为她是我的玫瑰”，因为真心地认真地付出过，才会不一样，才有感动，才会感觉更美，然而却也会更难忘记。
《他们最幸福》和《乖，摸摸头》是大冰以一背包客的身份，讲诉行走川藏过程中遇到的人和事，其中不乏感动。也许缺乏军人一般豪气的历练，“给你一碗酒，可以慰风尘”的越战老兵深深打动我；也许仅是屌丝一枚，椰子姑娘漂流记中“喂，这张床分我一半”的爱情结局令我羡慕。对长期浸淫在计算机领域的我，初读时，确实带来了很多的感动。然而，之后理性地细想，却发现在洒脱的文字底部透着一股子的矫情。也许故事是真的，却在行文中给原本的真实多了份刻意的渲染。所以大冰的书是那种适合缺乏感动的人初读的书，却不是沉浮世故的人耐读的书。
《货币战争5》只是看了一部分，找到这部书市因为想找一些关于从经济角度解读宋史的材料，所以只读了“北宋兴亡，铅华洗尽的沧桑”那一章。由于当时没做笔记，现只大致回忆一下逻辑。由于北宋在与西夏和辽的对抗中，战争对货币的需求增加，国家货币铸造增加。北宋每年新增的货币量，从995年的每年铸铜钱80万贯，逐步增加到1000年前后的125万贯，1007年的183万贯，1045年的300万贯，到宋神宗元丰三年（1080年）达到了506万贯的顶峰！货币的增加，一方面，促进了北宋真宗和仁宗时期《清明上河图》的繁荣，另一方面，到北宋后期后期宋神宗时期，也造成了急遽的通货膨胀。通货的结果，一方面，货币流入到少部分官僚及当时的“银行家”手中，进而发展成地主，形成宋朝的土地兼并；另一方面，在四川等地由于货币贬值，金属货币携带不便，开始出现中国的最早的纸币——交子。宋神宗时期的王安石变法，目的就是为了改变这种财富不均、土地兼并、财政枯竭的积贫积弱现象，然而，却失败了！
《吴晗说明史》《明朝那些事儿》《万历十五年》都是关于明史的记录，总体来说比较客观。虽然大部分历史学家对《万历十五年》推崇备至，几年前读过一次，这又看一次，却依然不是特别懂（之后再读）。倒觉得《吴晗说明史》和《明朝那些事》更白话。破土木堡之变，延续明朝历史近300年，“粉身碎骨浑不怕”两袖清风的于谦，顿时让自己又多了份正气，去到杭州，一定得去于谦墓瞻仰。也知道张居正是一个权臣，戚继光是贪官，而这些人的底子里却依然有为国为民的本性，人心的复杂岂是“好人坏人”两个词可以衡量！相较而言，“你是个好人，却并无用处“的海瑞就是一个异类。夏言的被迫害、严嵩的倒台、徐阶的隐忍奸诈、高拱专政、张居正的死后鞭尸，这一阶段变幻权利的斗争，对这些个人来说不过人世梦一场无不以提心掉胆的灾难结束，哪有老百姓平淡的向死而生来得舒坦。《明朝那些事》非常推荐一看，小说的品味却也不太失历史的严谨，最喜欢作者书后对读历史的观点，
很多人问，为什么看历史，很多人回答，以史为鉴。 现在我来告诉你，以史为鉴，是不可能的。 因为我发现，其实历史没有变化，技术变了，衣服变了，饮食变了，这都是外壳，里面什么都没变化，还是几千年前那一套，转来转去，该犯的错误还是要犯，该杀的人还是要杀，岳飞会死，袁崇焕会死，再过一千年，还是会死。 所有发生的，是因为它有发生的理由，能超越历史的人，才叫以史为鉴，然而我们终究不能超越，因为我们自己的欲望和弱点。 所有的错误，我们都知道，然而终究改不掉。
以史为镜也许可以，但以史为鉴就是一味饱含苦杏仁味的鸡汤，是一句屁话。所以也就不太在意能从历史中学到什么了。
《月亮与六便士》这种颠覆我价值观念的书就像一击重锤敲醒了定势思维的脑袋。月亮与六便士讲的是：斯特里克兰德，他突然放弃银行工作，抛妻弃子，远走巴黎，在穷困潦倒时他的朋友施特略夫不顾妻子反对救助他。施特略夫是一个十足善良的人，
有时候一个人的外貌同他的灵魂这么不相称，这实在是一件苦不堪言的事。施特略夫就是这样：他心里有罗密欧的热情，却生就一副托比·培尔契爵士的形体。他的禀性仁慈、慷慨，却不断闹出笑话来：他对美的东西从心眼里喜爱，但自己却只能创造出平庸的东西；他的感情非常细腻，但举止却很粗俗。他在处理别人的事务时很有手腕，但自己的事却弄得一团糟。大自然在创造这个人的时候，在他身上揉捏了这么多相互矛盾的特点，叫他面对着令他迷惑不解的冷酷人世，这是一个多么残忍的玩笑啊。
斯特里克兰德没有感激施特略夫，他夺取了朋友妻子，而后又抛弃了她，最终她因他而自尽时，他无半点同情，
“难道你不爱你的孩子们吗”？他说：“我对他们没有特殊感情”；
“难道你连爱情都不需要吗？”他说：“爱情只会干扰我画画”。
这样一个残酷、冷漠、自私、不尊重友情不尊重生命的人，最终却在他死后，他的画成为价值连城的艺术品。不尊重生活也可以有艺术，技术再好也未必表示他能有菩萨的心肠。斯特里克兰德就是这么一个人，
他好象是一个终生跋涉的朝香者，永远思慕着一块圣地。盘踞在他心头的魔鬼对他毫无怜悯之情。世上有些人渴望寻获真理，他们的要求非常强烈，为了达到这个目的，就是叫他们把生活的基础完全打翻，也在所不惜。思特里克兰德就是这样一个人；只不过他追求的是美，而不是真理。
我不禁想，自己到底欣赏的是斯特略夫的善良还是斯特里克兰德的为艺术而坚持的冷漠？我也不知道。
相比于《月亮与六便士》中“不需要爱情”的斯特里克兰德，《了不起的盖茨比》中的盖茨比则是脱离现实的过分深情。这是一个很悲伤的故事：
盖茨比年轻时与黛西热恋，因为没钱，黛西最后嫁给了富家子弟汤姆，而盖茨比到欧洲参加了第一次世界大战。战后归来，盖茨比通过非法的酒和食品买卖挣了大钱。他对黛西年年不忘，在黛西住处对面买了栋别墅。为吸引黛西的注意，盖茨比邀请明星好友，别墅里夜夜笙歌艳舞。后来，在尼克的安排下，盖茨比和已婚的黛西见了面，黛西的表情让盖茨比迷恋，盖茨比以为可以挽回昔日的恋情。汤姆也被邀请到别墅里，盖茨比、黛西、汤姆、尼克一起去纽约，盖茨比与黛西同一辆车，“于是在稍微凉快一点的暮色中向死亡驶去”。黛西因情绪激动开车撞死了汤姆的情妇玛特尔，肇事后逃走。玛特尔的丈夫威尔逊被汤姆挑拨，以为盖茨比是凶手，于是潜入盖茨比家中，杀死了盖茨比。威尔逊开枪自杀。事后，除了盖茨比的父亲，盖茨比昔日到别墅笙歌艳舞的好友唯恐避之不及，没有人来参加盖茨比的丧礼。黛西和汤姆携带行李逃去欧洲旅游。
站在盖茨比的别墅，能看到黛西住处有一盏灯，那是盖茨比盲目崇拜黛茜的脱离现实的梦想，
当我坐在那里缅怀那个古老的、未知的世界时，我也想到了盖茨比第一次认出了黛西的码头尽头的那盏绿灯时所感到的惊奇。他经历了漫长的道路才来到这片蓝色的草坪上，他的梦一定就像是近在眼前，他几乎不可能抓不住的。他不知道那个梦已经丢在他背后了，丢在这个城市那边那一片无垠的混饨之中不知什么地方了，那里合众国的黑黝黝的田野在夜色中向前伸展。 盖茨比信奉这盏绿灯，这个一年年在我们眼前渐渐远去的极乐的未来。它从前逃脱了我们的追求，不过那没关系――明天我们跑得更快一点，把胳臂伸得更远一点……总有一天…… 于是我们奋力向前划，逆流向上的小舟，不停地倒退，进入过去。
盖茨比对爱情幻想付出生命的追求，而盖茨比死后人世的无情，狰狞的现实在可怕的噩梦之间辗转反侧，就是一段爵士般的挽歌！记起小说中尼克的话，
盖茨比在我眼中有了生命，忽然之间从他那子宫般的毫无目的的豪华里分娩了出来。
《撒哈拉的故事》是三毛和荷西刚结婚在撒哈拉沙漠定居的故事。入洞房，久病成医，偷看人洗澡，羊从房顶掉下来……每个小故事，仅在撒哈拉沙漠里平淡而真实。摘录故事中一些感动过我的标注，
我在想，總有一天我們會死在這片荒原裡。
我在想——也許——也許是我潛意識裡總有想結束自己生命的欲望。所以——病就來了。
今天是回教開齋的節日，窗外碧空如洗，涼爽的微風正吹進來，夏日已經過去，沙漠美麗的秋天正在開始。
生命的過程，無論是陽春白雪，青菜豆腐，我都得嘗嘗是什麼滋味，才不枉來走這麼一遭啊！
撒哈拉沙漠是這麼的美麗，而這兒的生活卻是要付出無比的毅力來使自己適應下去啊！ 我沒有厭沙漠，我只是在習慣它的過程裡受到了小小的挫折。
到此结束，北京的生活也是这么的美丽，而这儿的生活却要付出无比的毅力来使自己适应下去！理性地思考后发现，我选择离开北京，所谓的厌恶了它的霾、它的生活方式只不过是借口，我只是在习惯它的过程中受到了小小的挫折。</description>
    </item>
  </channel>
</rss>
