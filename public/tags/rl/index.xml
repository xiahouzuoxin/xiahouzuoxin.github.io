<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>RL on Monkeyzx</title>
    <link>http://localhost:1313/tags/rl/</link>
    <description>Recent content in RL on Monkeyzx</description>
    <image>
      <title>Monkeyzx</title>
      <url>http://localhost:1313/papermod-cover.png</url>
      <link>http://localhost:1313/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 22 Mar 2019 19:47:48 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rethinking DQN</title>
      <link>http://localhost:1313/posts/rethinking-dqn/</link>
      <pubDate>Fri, 22 Mar 2019 19:47:48 +0800</pubDate>
      <guid>http://localhost:1313/posts/rethinking-dqn/</guid>
      <description>原文来自自己很早前写的一篇公众号文章：Rethinking DQN
Playing Atari with Deep Reinforcement Learning 为什么增加Replay Memory有效？
一个样本可能被多次随机采样到，在多次权重更新中被使用，多次学习，局部策略学习更有效 直接学习连续的游戏帧样本，前后帧之间存在强的相关性，从memory中随机采样打破了这种相关性 Replay Memory相当于使用Replay Memory长度时间的behaviour分布，避免使用当前时刻behaviour出现震荡和局部最优 针对第3点，避免陷入局部最优和震荡的情况（比如陷入 往左-&amp;gt;update Q-&amp;gt;max Qaction-&amp;gt;往左 这种局部循环），Replay Memory需要使用off-policy的策略，所以选择Q Learning
on-policy: when learning on-policy the current parameters determine the next data sample that the parameters are trained on off-policy: current parameters are different to those used to generate the sample DQN中因为要积累batch，使用Experience Replay，所以是典型off-policy的 DQN输入预处理方法：
RGB -&amp;gt; GrayScale resize，然后切掉游戏背景的黑边，最后size从(210,160)变成(84.84) 使用最近的4帧作为输入，因此输入为4x84x84 关于Reward：
由于原文准备在不同的游戏上使用相同的架构和学习参数，所以把正reward都设成1，负reward都设成0，0 reward不变。用这种截断reward的方式，限制了error的范围，从而可以使用相同的学习率参数 上面这种截断reward的方式对性能有一定的影响 优化方法：
RMSProp with batchsize=32 e-greedy的阈值概率：前面100 0000(1 million)帧从1到0.</description>
    </item>
  </channel>
</rss>
