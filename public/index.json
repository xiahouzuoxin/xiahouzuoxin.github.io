[{"content":"背景 想要用ML模型拟合业务，但实际中经常会遇到类似下面的场景：\n二手车价格预估：同一辆车，预估的价格与里程应该是严格负相关的； 保费预估：保额越高，保费越高； 理财产品中，比如余额宝，用户的利率敏感度预估（利率敏感度=用户余额变化/利率变化）：同一用户，理财产品利率越高，购买金额越高，余额越高； 如果直接用比如xgb或dnn模型拟合 X -\u0026gt; Y，\n$$ p_y=f(x), where\\ x = [x_c, x_p] $$ 其中f为xgb或dnn等具体模型，x_c为先验无关的特征（比如二手车里面的品牌和里程特征无关），x_p为先验相关的特征。由于噪声数据的存在以及模型中没有任何的相关性约束，模型最终的打分输出很难保证满足上面案例中的约束关系。\n自然想到的方法 一种自然想到的方法是，将f拆解成2部分：\n$$ p_y=f_c(x_c) + f_p(x_c,x_p) $$ 其中：\n$f_c$主要建模先验无关特征，可以是任意复杂模型，其输入只能是$x_c$； $f_p$单独建模先验信息，必须是满足先验约束的模型，一般是满足单调性的模型，比如LR、IsotonicRegression等，其输入包含$x_p$； 根据这个思路，二手车价格的建模就可以表达为：predicted car_value = 基础平均价格（baseclean_trade） + 里程相关的价格（predicted mieage_adjustment）。\n这种方法建模有个问题，数据集怎么构造？针对二手车预估这个问题，标签一般只有最终的车价，basecleantrade model和averagemileage模型的标签怎构造呢？\nbasecleantrade model：除里程相关特征（odometer，mileage等）外，其他特征group by计算平均价格作为标签，即同款车在不同里程下的平均价格作为标签； averagemileage：里程特征的平均作为标签，其他特征作为模型的输入； 从数据构造过程也可以看出，要采用这种方法的话，就一定得保证同款车有多个不同里程下的数据，这样效果才会好。\n进一步——End2End 上面需要建模多个模型，是否能把多个模型融合做到End 2 End训练呢？答案是在DNN中加入先验信息。\n同样对于二手车价格的建模，如下图，可以在DNN中针对里程这一项加入先验约束（mileageadjust module）。\n图里画的是线性约束，当然，线性约束效果最中预估效果上可能会有一点折扣，可以考虑survival/cox function作为先验约束函数能达到更平滑的效果。具体用什么约束函数，是通过数据分析得到的。\n根据这个思路，如果我要预估理财中的用户利率敏感度可以怎么做呢？把上面的思路顺一顺：\n分析历史数据中“利率变化\u0026quot;和“余额变化\u0026quot;的关系，如果是接近线性，那就可以用线性先验，如果只是单调则可以考虑survival/cox function；也可以结合一些业务/经济规律来定义先验，比如需求价格弹性； DNN建模，在模型中加入步骤1中得到的先验约束； 最终，利率敏感度建模可以类似下面这样（先验有点类似于需求价格弹性先验）。其中$Q_t$是用户在t日的余额，$r_t$是产品在t日的利率，$Q_{t-\u0026gt;t+N}$用户余额变化，$x_{t^+}$是用户利率变化，模型训练完后的$E_d$即可输出作为利率敏感度。\n扩展 其实了解过uplift因果模型的同学，应该很容易发现这里的建模方式和uplift中的s-learner很像，只是s-learner中一般treatment是二元变量，但是这里例子中的变量（比如利率，里程等）是连续型变量。\n所以要继续深入可以研究将uplift中的方法引入到先验建模场景中。\n","permalink":"http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1%E5%A6%82%E4%BD%95%E8%9E%8D%E5%85%A5%E5%85%88%E9%AA%8C%E4%BF%A1%E6%81%AF/","summary":"背景 想要用ML模型拟合业务，但实际中经常会遇到类似下面的场景：\n二手车价格预估：同一辆车，预估的价格与里程应该是严格负相关的； 保费预估：保额越高，保费越高； 理财产品中，比如余额宝，用户的利率敏感度预估（利率敏感度=用户余额变化/利率变化）：同一用户，理财产品利率越高，购买金额越高，余额越高； 如果直接用比如xgb或dnn模型拟合 X -\u0026gt; Y，\n$$ p_y=f(x), where\\ x = [x_c, x_p] $$ 其中f为xgb或dnn等具体模型，x_c为先验无关的特征（比如二手车里面的品牌和里程特征无关），x_p为先验相关的特征。由于噪声数据的存在以及模型中没有任何的相关性约束，模型最终的打分输出很难保证满足上面案例中的约束关系。\n自然想到的方法 一种自然想到的方法是，将f拆解成2部分：\n$$ p_y=f_c(x_c) + f_p(x_c,x_p) $$ 其中：\n$f_c$主要建模先验无关特征，可以是任意复杂模型，其输入只能是$x_c$； $f_p$单独建模先验信息，必须是满足先验约束的模型，一般是满足单调性的模型，比如LR、IsotonicRegression等，其输入包含$x_p$； 根据这个思路，二手车价格的建模就可以表达为：predicted car_value = 基础平均价格（baseclean_trade） + 里程相关的价格（predicted mieage_adjustment）。\n这种方法建模有个问题，数据集怎么构造？针对二手车预估这个问题，标签一般只有最终的车价，basecleantrade model和averagemileage模型的标签怎构造呢？\nbasecleantrade model：除里程相关特征（odometer，mileage等）外，其他特征group by计算平均价格作为标签，即同款车在不同里程下的平均价格作为标签； averagemileage：里程特征的平均作为标签，其他特征作为模型的输入； 从数据构造过程也可以看出，要采用这种方法的话，就一定得保证同款车有多个不同里程下的数据，这样效果才会好。\n进一步——End2End 上面需要建模多个模型，是否能把多个模型融合做到End 2 End训练呢？答案是在DNN中加入先验信息。\n同样对于二手车价格的建模，如下图，可以在DNN中针对里程这一项加入先验约束（mileageadjust module）。\n图里画的是线性约束，当然，线性约束效果最中预估效果上可能会有一点折扣，可以考虑survival/cox function作为先验约束函数能达到更平滑的效果。具体用什么约束函数，是通过数据分析得到的。\n根据这个思路，如果我要预估理财中的用户利率敏感度可以怎么做呢？把上面的思路顺一顺：\n分析历史数据中“利率变化\u0026quot;和“余额变化\u0026quot;的关系，如果是接近线性，那就可以用线性先验，如果只是单调则可以考虑survival/cox function；也可以结合一些业务/经济规律来定义先验，比如需求价格弹性； DNN建模，在模型中加入步骤1中得到的先验约束； 最终，利率敏感度建模可以类似下面这样（先验有点类似于需求价格弹性先验）。其中$Q_t$是用户在t日的余额，$r_t$是产品在t日的利率，$Q_{t-\u0026gt;t+N}$用户余额变化，$x_{t^+}$是用户利率变化，模型训练完后的$E_d$即可输出作为利率敏感度。\n扩展 其实了解过uplift因果模型的同学，应该很容易发现这里的建模方式和uplift中的s-learner很像，只是s-learner中一般treatment是二元变量，但是这里例子中的变量（比如利率，里程等）是连续型变量。\n所以要继续深入可以研究将uplift中的方法引入到先验建模场景中。","title":"机器学习建模——如何融入先验信息"},{"content":"为什么需要重读word2vec Word2vec原论文：\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781,2013. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. NIPS 2013. 其中1主要是描述CBOW和skip-gram相关方法，2描述了Negative Sampling以及训练word2vec的方法。为什么想要重读word2vec：\n推荐系统中，可以把item或user类比与word2vec中的word，很多i2i和u2i的方法都是借鉴的word2vec，以及召回中常用的NCE Loss和Sampled Softmax Loss都和word2vec相关；熟悉word2vec对了解召回，对如何像Aribnb KDD18和阿里EGES一样将word2vec改进应用到具体业务中，会非常有帮助； 召回热门样本的处理，也可以借鉴word2vec中热门词的处理思路； GPT的输入词是字符串，怎么表征输入到NN？当然，现在大家都很熟悉了embedding lookup table，embedding lookup table的其实最早差不多就是来自word2vec（更早影响没那么大暂且不提）；沿着这个拓展，因为词表太大，又在embedding前加了tokenizer过程； word2vec是老祖级别的语言模型，那么最新的GPT和它主要区别在哪里？ 以及初次看GPT代码时，发现GPT的输出是softmax，word2vec其实loss也是softmax loss。但为什么在word2vec里面的softmax就需要Negative sampling，而GPT里却不需要？ CBOW和skip-gram word2vec核心思想是通过训练神经网络，将word映射到高维embedding。其embedding的思想对推荐系统、以及Bert,GPT等都影响深远。Word2vec一般有两种训练方法：CBOW和skip-gram。\nCBOW是通过上下文（Context）学习当前word（BERT中的完形填空和这个很像），假设词与词之间相互独立，则对应的极大似然学习目标为：\n$$ E=-\\log p(w_t|w_{t-1},w_{t-2},..,w_{t-c},w_{t+1},w_{t+2},...,w_{t+c}) \\\\ = -\\log \\frac{e^{h \\cdot v'_o} }{\\sum_{j \\in V} e^{h \\cdot v'_j }} \\\\ = - h \\cdot v'_o + \\log {\\sum_{j \\in V} e^{h \\cdot v'_j}} $$ 其中，\nc是context窗口大小，在一个context内就是正样本，是个人工超参数，或根据一定规则动态调整； Loss函数使用的Softmax多分类，类目数即为词典大小V（语言模型里面，V大概几万-十万），即预估中心词是字典中的哪一个； h是CBOW的隐藏层输出，所有输入$w_{t-1},w_{t-2}\u0026hellip;,w_{t+1},w_{t+2}$通过lookup table查到embedding查询得到 $v_{t-1},v_{t-2}\u0026hellip;,v_{t+1},v_{t+2}$之后，再进行sum pooling的结果；即$h=1/C*(v_{t-1},v_{t-2},\u0026hellip;,v_{t+1},v_{t+2},\u0026hellip;)$，C为上下文长度；输入表达矩阵（lookup table）存储的就是word embedding的结果； v\u0026rsquo;是输出矩阵表达，注意在word2vec里面，和输入表达矩阵（即lookup table）不共享参数（GPT中的Head和输入Lookup Table是共享参数）； 模型假设 = h * v\u0026rsquo;：输入表征和预估词表征的点乘运算，可以理解为是输入context的embedding（即sum pooing结果）和输出v\u0026rsquo;的内积相似度，最终loss算的是这个相似度的softmax loss； Skip-gram与CBOW相反，它是则根据当前word来学习可能的上下文概率，对应的极大似然估计目标为：\n$$ E=-\\log p(w_{t-1},w_{t-2},...,w_{t-c},w_{t+1},w_{t+2},...,w_{t+c}|w_t) \\\\ =-\\log p(w_{t-1}|w_t,..,w_{t-c}|w_t,...,w_{t+1}|w_t,...,w_{t+c}|w_t) \\\\ =-\\log \\prod_{o \\in C} p(w_o|w_t) \\\\ =-\\sum_{o \\in C} \\log \\frac{e^{v_i \\cdot v'_o} }{\\sum_{j \\in V} e^{v_i \\cdot v'_j }} \\\\ =-\\sum_{o \\in C} {v_i \\cdot v'_o} + C \\log \\sum_{j \\in V} e^{v_i \\cdot v'_j } $$ 其中，\nLoss函数也是使用的Softmax多分类，类目数即为词典大小V；相比CBOW，这里最外层多了对Context的求和，C即为Context； $v_i$是输入word通过lookup table查到的embedding，没有也无需sum pooling操作，因为skip-gram输入只有一个词； 同样的，v\u0026rsquo;是输出矩阵表达，注意在word2vec里面，和输表达入矩阵（即lookup table）不一样； v_i * v\u0026rsquo; 点乘运算，可以理解为是当前词的embedding和Context里每个词的embedding计算相似度（注意这里输出词的embedding和输入词不是同一个词表）； 正样本为上下文内的样本，负样本为大词典V中的其他样本，loss函数为softmax。 Negative Sampling 到目前为止，已经有了CBOW和Skip-gram的原始优化目标，实际上不管是CBOW还是Skip-gram都是建模成词典大小的softmax多分类问题。但是有个很棘手的问题——计算复杂度。假设词表大小是V，隐藏层维度是H，则每次loss计算，softmax的分母就需要$O(V*H^2)$次的乘法，这是不可接受的。为了有效的优化目标，工程实现上有 Hierarchical Softmax 和 Negative Sampling的方法。Hierarchical Softmax在工业界用得不多，Negative Sampling相对更容易实现，所以这里只讨论Negative Sampling。\nNegative Sampling的Loss函数，一种方式是直接对负样本采样，比如包括自己就5类，直接在5类上计算softmax——Sampled Softmax Loss：\n$$ Sampled Softmax Loss= \\log \\frac {e^{v_i \\cdot v'_o}}{e^{v_i \\cdot v'_o} + \\sum_{j \\in S_{neg}} e^{v_i \\cdot v'_j}} $$ 另一种方式是采用NEC（Noise Contrastive Estimation）Loss，有文献证明NEC Loss可以近似上面的softmax loss。而工业界常用NEC的简化版本，NEG Loss：\n$$ NEG Loss=-log (\\frac {1}{1+e^{-v_i \\cdot v'_o}}) - \\sum_{j \\in S_{neg}} log (1 - \\frac {1}{1+e^{-v_i \\cdot v'_j}}) \\\\ =-log\\ \\sigma(v_i \\cdot v'_o) - \\sum_{j \\in S_{neg}} log\\ \\sigma(-v_i \\cdot v'_j) $$ 其中，\nS_neg为采样的负样本，一个正样本采样多个负样本； NEG Loss实际上就是计算sigmoid函数的BCE Loss，只是负样本是采样得到的； 我们可以从另一个视角来理解NEGLoss更容易：构造样本，中心词w与上下文c同属一个窗口，则\u0026lt;w,c\u0026gt;样本的label为1；随机采样一些上下文c\u0026rsquo;，则 \u0026lt;w,c\u0026rsquo;\u0026gt;的样本label为0。建模w和context的联合概率$p(w,c)$——二分类就是sigmoid函数，不同于skip-gram和CBOW里面的条件概率，对应Loss为：\n$$ NEGLoss = -\\log (\\prod_{(w,c) \\in D_p} p(D=1|w,c) + \\prod_{(w,c') \\in D_n} p(D=0|w,c') ] ) \\\\ = -\\sum_{(w,c) \\in D_p} \\log \\frac{1}{1+e^{-v_c \\cdot v_w}} - \\sum_{(w,c') \\in D_n} \\log (1 - \\frac{1}{1+e^{-v_c' \\cdot v_w}}) \\\\ = -\\sum_{(w,c) \\in D_p} \\log \\sigma(v_c \\cdot v_w) -\\sum_{(w,c') \\in D_n} \\log \\sigma(v_c' \\cdot v_w) $$ 其中，Dp是正样本集合，Dn是采样的负样本集合。\nSampled Softmax Loss和NEG Loss在推荐系统中应用非常广泛，尤其是推荐系统的召回模型中。\n高频词降采样 为了很好的完成对word embedding的训练，还有一个问题：对于\u0026rsquo;a\u0026rsquo;, \u0026rsquo;the\u0026rsquo;, \u0026lsquo;in\u0026rsquo;这种词，出现频率很高，但信息含量又很低，频繁的送入word2vec模型反而容易把模型带偏。为了处理这种imbalance的高频词，在训练时，word2vec采用下面的概率对word进行随机丢弃：\n$$ P(w_i)=1-\\sqrt{\\frac{t}{f(w_i)}} $$ 其中\nf(w_i)就是词w_i出现的频率。100个词里面出现10词，则=1/10，代表热度； t是一个人工选定的阈值（原文$10^{-5}$）； word2vec与推荐算法 item2vec 把item类比成word2vec中的word，就有了item2vec，利用item的vector进行i2i协同过滤。且item2vec中使用了和word2vec一样的高频词降采样方法对热门item打压。\nReal-time Personalization using Embeddings for Search Ranking at Airbnb Aribnb分别采用：\n用户click session的数据生成listing的embedding，捕捉短期的实时个性化； 用户booking session的数据生成user-type和listing-type的embedding，捕捉中长期用户兴趣； Listing embedding采用word2vec中的skip-gram模式（如下图），用户点击序列类比于word2vec的上下文，但是相比word2vec又做一些应用场景的适配：\n除了考虑上下文，如果有booking，在推荐中被认为是一个强信号，将booking也加到context中；因此其优化目标除了word2vec中的正负样本，还增加了booking listing的正样本（下面优化目的第3项）； 酒店预定一般具有地理特性，即用户一般一个session都在浏览同一地区的酒店，如果只采用word2vec中的随机负采样，引入的都是easy样本（直接通过地区信号就能分辨出负样本），因此还需要引入一些hard samples，从central listing的相同地区采样部分hard样本，构成下面优化目标中的第4项； Listing Embedding optimization target User-type和Listing-type Embedding也是skip-gram model，然后针对应用场景做特定的优化。\nEGES Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba\n将word2vec中文本序列的表达扩展到graph的表达，将多个用户的行为序列画在graph上，在graph上随机游走生成新的item序列（类比text context window），根据生成的item序列用Skip-gram学习item embedding表达。\nDSSM 各大厂里面有很多将双塔用于推荐召回的例子，其对应的loss和负采样方法都一定程度借鉴的word2vec。比如：\nMicrosoft: DSSM 2013：DSSM用于文档检索 Facebook: Embedding-based Retrieval in Facebook Search, 2020：详细介绍了Facebook基于Embedding召回的工程实践 Google Youtube: Sampling-bias-corrected neural modeling for large corpus item recommendations, 2019：Youtube双塔召回 Google Play: Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations, 2020：google play双塔召回 关于DSSM用于召回本身有很多工程细节，需要单独一篇文章去讲解。双塔最基本的结构基本如下：Query和Document的Embedding可以对应word2vec的输入表达和输出表达矩阵，只是word2vec的输入表达和输出表达都是word，而双塔模型中可能是i2i，也可能是u2i。\n不管如何，双塔模型的negative sampling和loss函数与word2vec很相似，比如：\nmicrosoft的dssm：negative sampled softmax loss，$ P(D|Q) = \\frac{exp(\\gamma R(Q,D))}{\\sum_{D\u0026rsquo; \\in D} exp(\\gamma R(Q,D\u0026rsquo;))} $，就是sampled softmax loss，增加了温度超参数$\\gamma$。\nword2vec与LLM BERT和GPT延续了word2vec表征一切的思路，用大数据pre-training就能得到NLP的非常好的表征。然后，只要针对具体业务场景的少量数据做fine-tuning就可以达到很不错的效果。BERT和GPT相比word2vec的主要优势在于，word2vec只考虑了单个词的表征，没有考虑词在不同上下文下的差异，比如 apple 在I have an apple laptop和I want eat apple里面含义是完全不同的，所以用于表征apple的embedding也应该是不同的。\n这里试着回答开头困惑的问题：\nGPT的输出是softmax，word2vec其实loss也是softmax loss。但为什么在word2vec里面的softmax就需要Negative sampling，而GPT里却不需要？\n答：因为word2vec里面softmax的分母，包含了输入表征和输出表征向量的inner product，且每次迭代都要计算V次，这才是计算量大的根源，复杂度$O(V*H^2)$；而GPT最后一层是lm_head（nn.Linear层），假设lm_head前的隐藏层输出为H，词典大小为V，复杂度仅为O(VH)，自然可以直接计算softmax loss。\n参考 Xin Rong. word2vec Parameter Learning Explained. 2016. Goldberg, Y. and Levy, O. (2014). word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv:1402.3722 [cs, stat]. arXiv: 1402.3722. Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013b). Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119. Grbovic, Mihajlo, and Haibin Cheng. \u0026ldquo;Real-time personalization using embeddings for search ranking at airbnb.\u0026rdquo; Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \u0026amp; data mining . 2018. nanoGPT. https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/model.py#L138C60-L138C107 ","permalink":"http://localhost:1313/posts/%E9%87%8D%E8%AF%BB%E7%BB%8F%E5%85%B8word2vec/","summary":"为什么需要重读word2vec Word2vec原论文：\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781,2013. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. NIPS 2013. 其中1主要是描述CBOW和skip-gram相关方法，2描述了Negative Sampling以及训练word2vec的方法。为什么想要重读word2vec：\n推荐系统中，可以把item或user类比与word2vec中的word，很多i2i和u2i的方法都是借鉴的word2vec，以及召回中常用的NCE Loss和Sampled Softmax Loss都和word2vec相关；熟悉word2vec对了解召回，对如何像Aribnb KDD18和阿里EGES一样将word2vec改进应用到具体业务中，会非常有帮助； 召回热门样本的处理，也可以借鉴word2vec中热门词的处理思路； GPT的输入词是字符串，怎么表征输入到NN？当然，现在大家都很熟悉了embedding lookup table，embedding lookup table的其实最早差不多就是来自word2vec（更早影响没那么大暂且不提）；沿着这个拓展，因为词表太大，又在embedding前加了tokenizer过程； word2vec是老祖级别的语言模型，那么最新的GPT和它主要区别在哪里？ 以及初次看GPT代码时，发现GPT的输出是softmax，word2vec其实loss也是softmax loss。但为什么在word2vec里面的softmax就需要Negative sampling，而GPT里却不需要？ CBOW和skip-gram word2vec核心思想是通过训练神经网络，将word映射到高维embedding。其embedding的思想对推荐系统、以及Bert,GPT等都影响深远。Word2vec一般有两种训练方法：CBOW和skip-gram。\nCBOW是通过上下文（Context）学习当前word（BERT中的完形填空和这个很像），假设词与词之间相互独立，则对应的极大似然学习目标为：\n$$ E=-\\log p(w_t|w_{t-1},w_{t-2},..,w_{t-c},w_{t+1},w_{t+2},...,w_{t+c}) \\\\ = -\\log \\frac{e^{h \\cdot v'_o} }{\\sum_{j \\in V} e^{h \\cdot v'_j }} \\\\ = - h \\cdot v'_o + \\log {\\sum_{j \\in V} e^{h \\cdot v'_j}} $$ 其中，","title":"重读经典——word2vec"},{"content":"SIM和ETA的问题 SIM通过类目从长序列中检索相关的items（hard-search）或者通过embedding inner product计算从长序列中检索最相似的topK个item；SIM的问题在 ETA 中讲过，主要是离线索引可能带来的在线target item embedding和离线的item embeddings不一致的问题； ETA在SIM基础上，通过对Long Behavior Sequence的item embeddings进行SimHash（LSH）之后，然后就可以将inner product的耗时计算转化为Hamming distance计算。从而大大降低了计算量，可以把检索topK的过程放到线上模型中，解决了SIM在离线不一致的问题；但是ETA依然需要通过一个Multi Head Target Attention得到最终的Long Behavior Sequence Embedding表达； 不管是SIM还是ETA，都是基于检索的方法从长序列（\u0026gt;1000）的候选items中选出最相关的topK，美团的这篇文章Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction 则是吸收了ETA中Hash函数的特点，但不使用检索的方法，不通过Multi Head Target Attention，直接得到用户Long Behavior Sequence的embedding表达——SDIM（Sampling-based Deep Interest Modeling）。\nHash-Based Sampling的用户行为建模 SimHash有Local Sentitive特性，也就是说空间距离相近的两个vector在Hash之后距离也 大概率 相近。SDIM就是利用这个原理：\n将序列的所有Item Embeddings和Target Embedding通过m个Hash函数进行Hash（比如下图m=4） Item Embedding每次Hash之后和Target Embedding落在同一个桶（Hash的LSH特性），则记为一次贡献； 进行m次Hash之后，统计每个item embedding的贡献，最后用L2 normalization进行归一化得到每个item embedding的权重 weights（按理L1也可以，但是作者说L2效果更好） 最后只需要将长序列的item embeddings通过weighted sum pooling求和就得到了Long Behavior Sequence的Embedding表达，完美避开了Multi Head Target Attention的操作； 当然，为了降低Hash冲突的风险，实际中需要将hash function数量m和Hash width设置得大一些。\n为了说明通过Hash计算每个item embedding权重的方法是有效的，作者通过理论分析和数据验证（如下图），Hash得到的weights和Target Attention得到的weight很接近。\n工程实现 Hash函数m和$\\tau$的选择：\nm理论上是越大越好，实验发现\u0026gt;48之后效果差不多了； 而$\\tau$并不是越大越好，太大了只能是极其相关的item才能贡献权重，太小了则回引入噪声items，原文的实验是=3左右； m的选择 $t$的选择 总结一下：自我感觉这篇工作还是比较优雅简洁，易工程实现的，对旧系统的改造不大。\n","permalink":"http://localhost:1313/posts/%E8%B6%85%E9%95%BF%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1sdim/","summary":"SIM和ETA的问题 SIM通过类目从长序列中检索相关的items（hard-search）或者通过embedding inner product计算从长序列中检索最相似的topK个item；SIM的问题在 ETA 中讲过，主要是离线索引可能带来的在线target item embedding和离线的item embeddings不一致的问题； ETA在SIM基础上，通过对Long Behavior Sequence的item embeddings进行SimHash（LSH）之后，然后就可以将inner product的耗时计算转化为Hamming distance计算。从而大大降低了计算量，可以把检索topK的过程放到线上模型中，解决了SIM在离线不一致的问题；但是ETA依然需要通过一个Multi Head Target Attention得到最终的Long Behavior Sequence Embedding表达； 不管是SIM还是ETA，都是基于检索的方法从长序列（\u0026gt;1000）的候选items中选出最相关的topK，美团的这篇文章Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction 则是吸收了ETA中Hash函数的特点，但不使用检索的方法，不通过Multi Head Target Attention，直接得到用户Long Behavior Sequence的embedding表达——SDIM（Sampling-based Deep Interest Modeling）。\nHash-Based Sampling的用户行为建模 SimHash有Local Sentitive特性，也就是说空间距离相近的两个vector在Hash之后距离也 大概率 相近。SDIM就是利用这个原理：\n将序列的所有Item Embeddings和Target Embedding通过m个Hash函数进行Hash（比如下图m=4） Item Embedding每次Hash之后和Target Embedding落在同一个桶（Hash的LSH特性），则记为一次贡献； 进行m次Hash之后，统计每个item embedding的贡献，最后用L2 normalization进行归一化得到每个item embedding的权重 weights（按理L1也可以，但是作者说L2效果更好） 最后只需要将长序列的item embeddings通过weighted sum pooling求和就得到了Long Behavior Sequence的Embedding表达，完美避开了Multi Head Target Attention的操作； 当然，为了降低Hash冲突的风险，实际中需要将hash function数量m和Hash width设置得大一些。","title":"超长行为序列建模SDIM"},{"content":"SIM的问题 前面介绍过阿里巴巴超长行为序列建模的方法SIM是two-stage，在预估的时候，SIM先通过target item从长行为序列中通过类目（hard-search）或item embedding相似度（soft-search）检索出top-K，再送到主模型中做target attention。由于类目索引和item embedding索引的构建是离线的，而CTR主模型很多时候需要online learning，这种情况下就造成用于检索的目标item embedding和离线构建的item embedding不在同一空间（离线的item索引存在滞后）。\nEnd-to-End User Behavior Retrieval in Click-Through Rate Prediction Model 是阿里主搜团队 基于此出发提出了End-to-end Target Attention - ETA，GMV提升3%。我们回到长序列问题本身，为啥不能像DIN/DIEN一样直接对超长序列做target attention——因为inference time吃不消（target attention需要对长序列中的每个item emebedding做inner product）。那么有没有其它方法可以快速从长序列中检索出top-K，又不需要像SIM那样最终要用到比如faiss做快速的索引？\nETA就想到了SimHash！通过将long behavior sequence item embeddings进行SimHash后，就能通过Hamming distance替代掉inner product，从而缓解长序列中inner product所带来的计算量问题。\nEmbedding SimHash后能快速的进行TopK检索 SimHash算法用于将每个item embedding映射成一个二进制的串（最终可以保存成一个整数），之前主要用于长文本相似度的比较。这里应用SimHash相当于把embedding当作长文本。\n本文使用的SimHash伪代码如下，代码中共进行m次Hash，Hash的width=2即每次Hash返回二值0或1的$sig_k[i]$：\n只要满足下面的条件，就能保证最终的计算复杂度能够降低：\n映射后的向量相似度计算函数的复杂度比inner product低（映射后的二进制向量用Hamming distance计算相似度，复杂度 \u0026lt; inner product）； 映射后的向量长度比原item embedding长度小（m\u0026lt;d）； Hash映射函数的复杂度不高；从伪代码看，Hash过程也需要计算内积，怎么感觉比inner product还高，Table 2里面的ETA的Retrieval complexity貌似也没把Hash函数的计算量考虑进去？主要原因是：在inference阶段，模型训练好之后，item embedding都是lookup table，那我们可以把lookup table里面的所有embeddings都提前计算好它的$sig_k$，然后和Embedding Table以及模型一起存储下来，最终只要根据item查表得到$sig_k$就行。所以Inference的时候，理论上Hash函数的时间可以忽略不计； 下图是采用SimHash之后，ETA对各种检索方法的时间复杂度、检索信息一致性的对比：相比直接inner product，ETA通过对item embedding做SimHash，然后用Hamming distance相似度函数进行检索。\n有几个问题：\nSimHash把d=128维度的embedding映射到了m=4维，且通过Hamming distance代替了inner product计算，这也就是ETA为什么能降低计算复杂度的核心原因。那如何保证映射后m=4维的向量后原d=128 embedding的信息不会丢失呢？SimHash有一个特殊的性质——locality sensitive hashing，即局部敏感哈希，也就是说空间上相近的两个embedding做完Hash之后位置依然大概率接近。 如果是SimHash映射到m=4然后再计算相似度，那为什么不建embedding lookup table的时候就让d=4？核心是因为这里的复杂度降低，是将inner product计算替换成了Hamming distance计算，Hamming distance通过异或XOR运算复杂度可以控制在O(1)，所以不管怎么降低embedding dim都比较难达到Hamming distance这么低的计算复杂度。 ETA (End-to-end Target Attention) ETA结构左边就是SimHash的应用，具体的：\n训练时： 得到Long-term User Behavior Sequence的item_ids的embeddings $e_{k+1}\u0026hellip;e_n$，和Target Item的embedding $e_t$； 对每个embeddings和target embedding计算SimHash，得到每个embedding对应的m维的二进制向量；训练时SimHash的计算开销还是在的； 用Hamming距离计算Target二进制向量和Behavior Sequence中的每个embedding距离，排序的到TopK ——注意这步top-K排序需要用计数排序或快排，才能将复杂度做到O(L*B*1)满足系统的性能要求； 训练好之后将所有embedding table的SimHash的结果的存成lookup table，和模型一起打包部署； 预估时： Long-term User Behavior Sequence的item_ids直接查询lookup table得到到每个item对应的m维的二进制向量； Hamming距离计算相似度； 排序得到TopK结果，Multi-Head Target Attention接入到主CTR模型中； 工程实现 SimHash不参与反向传播，只用于检索TopK，但是输入长序列中item的embeddings会随着模型更新而更新（注意，SIM中这部分是延迟更新的）； Embeddings Hash的结果可以存成lookup table让SimHash过程在inference阶段基本没有计算开销，且因为每个结果都是0或1二进制，所以可以直接存成整数，进一步大大降低memory消耗； 关于效果：文章中没有和SIM(soft) + timeinfo做对比。SIM主要问题在于检索过程离线embedding的延迟；但ETA有没有自己的问题？当然有，embedding hash过程虽然说是LSH，但或多或少还是存在一些embedding的信息丢失，导致topK检索的精度打些折扣。\nAUC效果 AB实验效果 ","permalink":"http://localhost:1313/posts/%E8%B6%85%E9%95%BF%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1eta/","summary":"SIM的问题 前面介绍过阿里巴巴超长行为序列建模的方法SIM是two-stage，在预估的时候，SIM先通过target item从长行为序列中通过类目（hard-search）或item embedding相似度（soft-search）检索出top-K，再送到主模型中做target attention。由于类目索引和item embedding索引的构建是离线的，而CTR主模型很多时候需要online learning，这种情况下就造成用于检索的目标item embedding和离线构建的item embedding不在同一空间（离线的item索引存在滞后）。\nEnd-to-End User Behavior Retrieval in Click-Through Rate Prediction Model 是阿里主搜团队 基于此出发提出了End-to-end Target Attention - ETA，GMV提升3%。我们回到长序列问题本身，为啥不能像DIN/DIEN一样直接对超长序列做target attention——因为inference time吃不消（target attention需要对长序列中的每个item emebedding做inner product）。那么有没有其它方法可以快速从长序列中检索出top-K，又不需要像SIM那样最终要用到比如faiss做快速的索引？\nETA就想到了SimHash！通过将long behavior sequence item embeddings进行SimHash后，就能通过Hamming distance替代掉inner product，从而缓解长序列中inner product所带来的计算量问题。\nEmbedding SimHash后能快速的进行TopK检索 SimHash算法用于将每个item embedding映射成一个二进制的串（最终可以保存成一个整数），之前主要用于长文本相似度的比较。这里应用SimHash相当于把embedding当作长文本。\n本文使用的SimHash伪代码如下，代码中共进行m次Hash，Hash的width=2即每次Hash返回二值0或1的$sig_k[i]$：\n只要满足下面的条件，就能保证最终的计算复杂度能够降低：\n映射后的向量相似度计算函数的复杂度比inner product低（映射后的二进制向量用Hamming distance计算相似度，复杂度 \u0026lt; inner product）； 映射后的向量长度比原item embedding长度小（m\u0026lt;d）； Hash映射函数的复杂度不高；从伪代码看，Hash过程也需要计算内积，怎么感觉比inner product还高，Table 2里面的ETA的Retrieval complexity貌似也没把Hash函数的计算量考虑进去？主要原因是：在inference阶段，模型训练好之后，item embedding都是lookup table，那我们可以把lookup table里面的所有embeddings都提前计算好它的$sig_k$，然后和Embedding Table以及模型一起存储下来，最终只要根据item查表得到$sig_k$就行。所以Inference的时候，理论上Hash函数的时间可以忽略不计； 下图是采用SimHash之后，ETA对各种检索方法的时间复杂度、检索信息一致性的对比：相比直接inner product，ETA通过对item embedding做SimHash，然后用Hamming distance相似度函数进行检索。\n有几个问题：\nSimHash把d=128维度的embedding映射到了m=4维，且通过Hamming distance代替了inner product计算，这也就是ETA为什么能降低计算复杂度的核心原因。那如何保证映射后m=4维的向量后原d=128 embedding的信息不会丢失呢？SimHash有一个特殊的性质——locality sensitive hashing，即局部敏感哈希，也就是说空间上相近的两个embedding做完Hash之后位置依然大概率接近。 如果是SimHash映射到m=4然后再计算相似度，那为什么不建embedding lookup table的时候就让d=4？核心是因为这里的复杂度降低，是将inner product计算替换成了Hamming distance计算，Hamming distance通过异或XOR运算复杂度可以控制在O(1)，所以不管怎么降低embedding dim都比较难达到Hamming distance这么低的计算复杂度。 ETA (End-to-end Target Attention) ETA结构左边就是SimHash的应用，具体的：","title":"超长行为序列建模ETA"},{"content":"为什么推荐需要超长行为序列？ 试想这么种场景，某个用户4个月前在某家店买了厕纸，2个月前又买了厕纸，然后中间又浏览了成千上万的其他东西，到现在又隔了两个月，应不应该推厕纸？\n然而，对于模型来说，因为2个月前到现在中间浏览了成千上万的其他东西，而DIN, DIEN, MIMN只能建模1000以下的行为序列，所以早就把厕纸从行为用户历史行为序列中剔除出去了。\n所以：\n第一，通过超长行为序列可以捕获用户长周期里面周期性或规律性的购买行为 第二，短序列只能刻画用户短时的兴趣偏好（买了几次盲盒，但无法确定是否二次元用户），但通过超长序列可以刻画用户的长期偏好（比如，除了最近几次的盲盒还买了很多手办，二次元用户） 任何事情也都有利弊，即使计算量不是问题，但直接把用户所有的历史行为序列到模型中，除了会引入有用信息，同时也引入了许多的噪音。如何从用户长历史中剔除噪声，提取有用的信息？SIM在一定程度上就是在解决这个问题。\n题外话：LLM里面的RGA，不是也为了解决这种问题吗？\nSIM Paper: Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction\nSIM是Two-stage Model， 包括GSU和ESU阶段。注意，主结构中依然保留了User short-time behaviors的输入，在短时序的基础上做增量，毕竟最近的行为序列才是最重要的。\nGeneral Search Unit (GSU) GSU，通过线性时间复杂度，用于从原始超长序列中检索出top-K相关的子行为。文中介绍了两种GSU的方式，soft-search和hard-search。两种搜索的差别主要在序列中的item和目标item的相似度的计算上，\n$$ r_i = \\left\\{ \\begin{aligned} sign (C_i=C_a) \u0026 \u0026 hard-search \\\\ (W_be_i) \\cdot (W_ae_a)^T \u0026 \u0026 soft-search \\end{aligned} \\right. $$ 其中，$C_a$是目标item的category，$C_i$是序列中behavior item的category，即只从长序列中筛选同category的items；soft-search则是直接通过behavior item embedding和目标item embedding的内积（inner product）计算相似度，SIM使用的检索算法是ALSH（这也是召回里面常用的user向量检索item向量的方法）。\n关于soft-search的item embedding：\n不能直接使用short behavior sequence的embedding，因为不在一个表达空间，embedding差异会比较大；所以才有了上图中左边Soft Search Training辅助任务； Soft Search在原文中是和后面的ESU一起训练，得到behavior item embedding矩阵，线上serving的时候通过类似i2i召回，内积检索得到topK的item；但是，一起训练成本太高，behavior item的长期表达在短期内变化不大，这里是否可以定期离线训练？ soft-search training的技巧：当life long sequential behavior实在太长的时候（内存放不下），可以随机采样子行为序列，对最终item embedding的分布影响不大； hard-search或soft-search的topK结果送到后续的ESU。\nExact Search Unit (ESU) ESU就是对TopK做multi-head attention，\n$$ att_{score}^i=softmax(W_{bi}z_b \\cdot W_{ai}e_a) \\\\ head_i=att_{score}^i z_b $$ 最终的长时间序列表达为多个head的concat：$U_{lt}=concat(head_1,\u0026hellip;,head_q)$，剩下2个问题：\n为什么是multi-head？为了从长序列中提取多兴趣，长序列中一般包含多兴趣（比如电子爱好者，同时可能是二次元爱好者）；\n$z_b$ 是什么，不是behavior item的embedding $e_b$么？z_b是 behavior item embedding 和 behavior item与target item时间差 的embedding concat，这里 behavior item与target item的时间差 这个timeinfo信息也很重要，从原文实验看，hard-search + timeinfo的提升基本和soft-search相当，千分之二的提升；\n题外话：timeinfo是连续or离散的，如果是连续的，怎么embedding呢？\n好了，剩下的就是把 ESU multi-head attention的结果 concat到短时序中进行训练了（soft-search条件允许是联合训练，hard-search部分不涉及训练过程）。\n工程上的实现问题 工程问题：\nAlibaba广告 精排打分延迟 \u0026lt; 30ms，有些场景甚至更短10ms内； 在高并发的情况下（淘宝1亿+用户，峰值qps可能几十亿），用户超长行为的embeddings，在打分时内存的压力也是个问题； 针对问题1，原文离线实验发现soft-search和hard-search效果差不多（soft-search好2个千分点），最终选择上线上线hard-search + timeinfo；\n针对问题2，把用户长行为序列建成KKV索引（Key-Key-Value）——User Behavior Tree，提前存储到数据库，模型在线hard-search直接从数据库中根据Key=user_id, Key=target_category索引得到ESU的输入topK；且通过索引的构建之后，hard-search的O(k)时间复杂度直接缩短到O(1)即在线serving长时序基本没有额外的开销。\n另外，其实对于soft-search的上线，检索过程可以完全复用召回倒排索引（Inverted index）的框架。\n","permalink":"http://localhost:1313/posts/%E8%B6%85%E9%95%BF%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1sim/","summary":"为什么推荐需要超长行为序列？ 试想这么种场景，某个用户4个月前在某家店买了厕纸，2个月前又买了厕纸，然后中间又浏览了成千上万的其他东西，到现在又隔了两个月，应不应该推厕纸？\n然而，对于模型来说，因为2个月前到现在中间浏览了成千上万的其他东西，而DIN, DIEN, MIMN只能建模1000以下的行为序列，所以早就把厕纸从行为用户历史行为序列中剔除出去了。\n所以：\n第一，通过超长行为序列可以捕获用户长周期里面周期性或规律性的购买行为 第二，短序列只能刻画用户短时的兴趣偏好（买了几次盲盒，但无法确定是否二次元用户），但通过超长序列可以刻画用户的长期偏好（比如，除了最近几次的盲盒还买了很多手办，二次元用户） 任何事情也都有利弊，即使计算量不是问题，但直接把用户所有的历史行为序列到模型中，除了会引入有用信息，同时也引入了许多的噪音。如何从用户长历史中剔除噪声，提取有用的信息？SIM在一定程度上就是在解决这个问题。\n题外话：LLM里面的RGA，不是也为了解决这种问题吗？\nSIM Paper: Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction\nSIM是Two-stage Model， 包括GSU和ESU阶段。注意，主结构中依然保留了User short-time behaviors的输入，在短时序的基础上做增量，毕竟最近的行为序列才是最重要的。\nGeneral Search Unit (GSU) GSU，通过线性时间复杂度，用于从原始超长序列中检索出top-K相关的子行为。文中介绍了两种GSU的方式，soft-search和hard-search。两种搜索的差别主要在序列中的item和目标item的相似度的计算上，\n$$ r_i = \\left\\{ \\begin{aligned} sign (C_i=C_a) \u0026 \u0026 hard-search \\\\ (W_be_i) \\cdot (W_ae_a)^T \u0026 \u0026 soft-search \\end{aligned} \\right. $$ 其中，$C_a$是目标item的category，$C_i$是序列中behavior item的category，即只从长序列中筛选同category的items；soft-search则是直接通过behavior item embedding和目标item embedding的内积（inner product）计算相似度，SIM使用的检索算法是ALSH（这也是召回里面常用的user向量检索item向量的方法）。\n关于soft-search的item embedding：\n不能直接使用short behavior sequence的embedding，因为不在一个表达空间，embedding差异会比较大；所以才有了上图中左边Soft Search Training辅助任务； Soft Search在原文中是和后面的ESU一起训练，得到behavior item embedding矩阵，线上serving的时候通过类似i2i召回，内积检索得到topK的item；但是，一起训练成本太高，behavior item的长期表达在短期内变化不大，这里是否可以定期离线训练？ soft-search training的技巧：当life long sequential behavior实在太长的时候（内存放不下），可以随机采样子行为序列，对最终item embedding的分布影响不大； hard-search或soft-search的topK结果送到后续的ESU。","title":"超长行为序列建模SIM"},{"content":"原文来自自己很早前写的一篇公众号文章：Rethinking DQN\nPlaying Atari with Deep Reinforcement Learning 为什么增加Replay Memory有效？\n一个样本可能被多次随机采样到，在多次权重更新中被使用，多次学习，局部策略学习更有效 直接学习连续的游戏帧样本，前后帧之间存在强的相关性，从memory中随机采样打破了这种相关性 Replay Memory相当于使用Replay Memory长度时间的behaviour分布，避免使用当前时刻behaviour出现震荡和局部最优 针对第3点，避免陷入局部最优和震荡的情况（比如陷入 往左-\u0026gt;update Q-\u0026gt;max Qaction-\u0026gt;往左 这种局部循环），Replay Memory需要使用off-policy的策略，所以选择Q Learning\non-policy: when learning on-policy the current parameters determine the next data sample that the parameters are trained on off-policy: current parameters are different to those used to generate the sample DQN中因为要积累batch，使用Experience Replay，所以是典型off-policy的 DQN输入预处理方法：\nRGB -\u0026gt; GrayScale resize，然后切掉游戏背景的黑边，最后size从(210,160)变成(84.84) 使用最近的4帧作为输入，因此输入为4x84x84 关于Reward：\n由于原文准备在不同的游戏上使用相同的架构和学习参数，所以把正reward都设成1，负reward都设成0，0 reward不变。用这种截断reward的方式，限制了error的范围，从而可以使用相同的学习率参数 上面这种截断reward的方式对性能有一定的影响 优化方法：\nRMSProp with batchsize=32 e-greedy的阈值概率：前面100 0000(1 million)帧从1到0.1线性变化，后面帧保持0.1不变 Replay memory的大小：最新的1 million帧 frame-skipping technique 怎么看训练结果好坏，训练时要打印什么东西观察收敛情况？\nthe total reward the agent collects in an episode or game averaged over a number of games，即每个episode（每一局）的reward变化。该度量方法结果会存在很大的noise或者说震荡，比如下面的左边两张图 更稳定的度量方式是：the policy’s estimated action-value function Q, which provides an estimate of how much discounted reward the agent can obtain by following its policy from any given state。如下面的右边两张图，即Q* Human-level control through deep reinforcement learning 在上一篇的基础上改进：target network和Qnetwork使用不同的参数\n大部分处理方式和上篇文章一样，增加了1点：\nerror term clip：error截断到-1到1之间。这种处理的效果类似于使用了加绝对值的loss函数 思考：使用HuberLoss是不是可以起到限制loss发散的效果？ 另外，这篇文章附录部分有一个表格描述使用的参数，实训过程可能有参考价值，\nDeep Reinforcement Learning with Double Q-learning DQN里面的核心其实是Q网络的标签y计算的问题，而y的计算是通过Bellman公式得到的。 最早的Bellman公式其实是计算期望（从某个状态s可切换到多个下一状态s\u0026rsquo;），\n但在Nature和Atrai文章中，因为都是使用s\u0026rsquo;中的某一个样本来训练，即(s,a,s\u0026rsquo;,r)四元组，所以 y的计算就变成上面划横向的部分。 Bellman公式很容易理解：预测从下一状态开始到结束q(s,a)，就是从s-\u0026gt;s\u0026rsquo;的reward+下一状态之后的最大q(s\u0026rsquo;,a')\n在DQN中这里的q都是是使用target net计算的\nDDQN则实际上是对上面公式的改进。DDQN认为Bellman中直接使用target net计算下一(s\u0026rsquo;,a\u0026rsquo;)的q，容易与y耦合。因此改成online的Qnet来选择下一状态action计算Q\n注意上面的θ和θ-分别代表oneline network和target network。\nPrioritized Experience Replay 比较于DDQN，PER改进Experience Replay\n原文根据TD-error给结果一个抽中的概率，从下面算法看，TD-error的计算跟DQN中loss的计算基本一致\n","permalink":"http://localhost:1313/posts/rethinking-dqn/","summary":"原文来自自己很早前写的一篇公众号文章：Rethinking DQN\nPlaying Atari with Deep Reinforcement Learning 为什么增加Replay Memory有效？\n一个样本可能被多次随机采样到，在多次权重更新中被使用，多次学习，局部策略学习更有效 直接学习连续的游戏帧样本，前后帧之间存在强的相关性，从memory中随机采样打破了这种相关性 Replay Memory相当于使用Replay Memory长度时间的behaviour分布，避免使用当前时刻behaviour出现震荡和局部最优 针对第3点，避免陷入局部最优和震荡的情况（比如陷入 往左-\u0026gt;update Q-\u0026gt;max Qaction-\u0026gt;往左 这种局部循环），Replay Memory需要使用off-policy的策略，所以选择Q Learning\non-policy: when learning on-policy the current parameters determine the next data sample that the parameters are trained on off-policy: current parameters are different to those used to generate the sample DQN中因为要积累batch，使用Experience Replay，所以是典型off-policy的 DQN输入预处理方法：\nRGB -\u0026gt; GrayScale resize，然后切掉游戏背景的黑边，最后size从(210,160)变成(84.84) 使用最近的4帧作为输入，因此输入为4x84x84 关于Reward：\n由于原文准备在不同的游戏上使用相同的架构和学习参数，所以把正reward都设成1，负reward都设成0，0 reward不变。用这种截断reward的方式，限制了error的范围，从而可以使用相同的学习率参数 上面这种截断reward的方式对性能有一定的影响 优化方法：\nRMSProp with batchsize=32 e-greedy的阈值概率：前面100 0000(1 million)帧从1到0.","title":"Rethinking DQN"},{"content":" 这些年我一直提醒自己一件事情，千万不要自己感动自己。大部分人看似的努力，不过是愚蠢导致的。什么熬夜看书到天亮，连续几天只睡几小时，多久没放假了，如果这些东西也值得夸耀，那么富士康流水线上任何一个人都比你努力多了。人难免天生有自怜的情绪，唯有时刻保持清醒，才能看清真正的价值在哪里。\n——于宙TEDx演讲：我们这一代人的困惑\n平时总拿追求安逸平凡作为生活的目标，其实是给自己懒惰找的借口而已。为什么要读书，下面是在知乎里看到的答案，\n我读过很多书，但后来大部分都被我忘记了，那阅读的意义是什么？\n答：当我还是个孩子的时候，我吃过很多食物，现在已经记不起来吃过什么了。但可以肯定的是，它们中的一部分已经长成为我的骨头和肉。\n2015年，因为经历，才深深体会了宫崎骏的一句话，\n你在的城市，天下起了雨，很想问你有没有带伞，可我却忍住了，因为我怕你说没带，而我却无能为力。\n为缓解这种忍耐的情绪，在下半年实习找工作之余，增加了一些读书量，如下图为2015年7月份以来的读书单。\n其中有一些是极力推荐的。我是个东坡迷，相较于林语堂的《苏东坡传》，感觉李一冰的更客观更详细——大部分以史学材料、东坡诗词和东坡同时代的文人札记（或书信等）为基础。东坡那“如食之有蝇，吐之乃已”的性格，即使在现代也是“一肚子的不合时宜”，估计也会沦落到晓松老师之流。然而，东坡身兼儒释道的正派而又不失率真豁达，才气逼人却是晓松老师不能比的。\n《目送》是台湾作家龙应台在老年时去回顾中年的感悟。在文中她写了生命中两件“小事”：儿子十六岁到美国当交换学生，在机场，她看着儿子通过护照检查、进入海关，背影倏地消失，没有回头；多年后，她父亲在医院的最后时光，她又看着轮椅上被护士推回房的父亲背影。通过这些事，她开始理解个人生命中最私密、最深埋、最不可言喻的“伤逝”和“舍”，并在文中写道：\n我慢慢地、慢慢地了解到，所谓父女母子一场，只不过意味着，你和他的缘分就是今生今世不断地在目送他的背影渐行渐远。\n虽还没到那个年纪，体悟不算太多，但也无法抗拒作者文字的朴素深沉。我记得这么一段描述，\n「火车突然停了，」母亲说，「车顶上趴着一堆人，有一个女的说憋不住了，无论如何要上厕所，就爬下来，她的小孩儿还留在车顶上头，让人家帮她抱一下。没想到，她一下来，车就动了。」\n小的时候，有一次和妈妈外出，也经历了同样的窘境。妈妈去上厕所还没回来，车就动了，那时候的我胆小，也不知道怎么和司机说，然后就一直害怕，“妈妈丢了，妈妈丢了…………”\n在我们整个成长的过程里，谁，教过我们怎么去面对痛苦、挫折、失败？它不在我们的家庭教育里，它不在小学、中学、大学的教科书或课程里，它更不在我们的大众传播里。家庭教育、学校教育、社会教育只教我们如何去追求卓越，从砍樱桃的华盛顿、悬梁刺股的孙敬、苏秦到平地起楼的比尔?盖茨，都是成功的典范。即使是谈到失败，目的只是要你绝地反攻，再度追求出人头地，譬如越王句践的卧薪尝胆，洗雪耻辱，譬如哪个战败的国王看见蜘蛛如何结网，不屈不挠。 我们拚命地学习如何成功冲刺一百米，但是没有人教过我们：你跌倒时，怎么跌得有尊严；你的膝盖破得血肉模糊时，怎么清洗伤口、怎么包扎；你痛得无法忍受时，用什么样的表情去面对别人；你一头栽下时，怎么治疗内心淌血的创痛，怎么获得心灵深层的平静，心像玻璃一样碎了一地时，怎么收拾？\n也成为一种教育悲哀的阐述——只教会如何坚持如何的努力，却没有告诉我们如何去缓解努力过程中带来的压力。\n《小王子》里记得有那么一段话居然影响到我写简历，\n如果你对大人们说：“我看到一幢玫瑰色的砖盖成的漂亮的房子，它的窗户上有天竺葵，屋顶上还有鸽子…”他们怎么也想象不出这种房子有多么好。必须对他们说：“我看见了一幢价值十万法郎的房子。”那么他们就会惊叫道：“多么漂亮的房子啊！”\n那就是，写简历的时候能用数字说话就用数字说话，别BB“啊我那个项目做得多好啊”你还不如直接说“我做的那个项目拉了100万投资”。我本来是寻找童真的，却收获到一点利益世俗。但是，却也有很多地方感动到心跳加速，比如小王子到玫瑰盛开的花园，拿玫瑰花园里的花和自己星球上的玫瑰做对比，\n“你们很美，但你们是空虚的。”小王子仍然在对她们说，“没有人能为你们去死。当然喽，我的那朵玫瑰花，一个普通的过路人以为她和你们一样。可是， 她单独一朵就比你们全体更重要，因为她是我浇灌的。因为她是我放在花罩中的。 因为她是我用屏风保护起来的。因为她身上的毛虫（除了留下两三只为了变蝴蝶 而外）是我除灭的。因为我倾听过她的怨艾和自诩，甚至有时我聆听着她的沉默。 因为她是我的玫瑰。”\n“因为她是我的玫瑰”，因为真心地认真地付出过，才会不一样，才有感动，才会感觉更美，然而却也会更难忘记。\n《他们最幸福》和《乖，摸摸头》是大冰以一背包客的身份，讲诉行走川藏过程中遇到的人和事，其中不乏感动。也许缺乏军人一般豪气的历练，“给你一碗酒，可以慰风尘”的越战老兵深深打动我；也许仅是屌丝一枚，椰子姑娘漂流记中“喂，这张床分我一半”的爱情结局令我羡慕。对长期浸淫在计算机领域的我，初读时，确实带来了很多的感动。然而，之后理性地细想，却发现在洒脱的文字底部透着一股子的矫情。也许故事是真的，却在行文中给原本的真实多了份刻意的渲染。所以大冰的书是那种适合缺乏感动的人初读的书，却不是沉浮世故的人耐读的书。\n《货币战争5》只是看了一部分，找到这部书市因为想找一些关于从经济角度解读宋史的材料，所以只读了“北宋兴亡，铅华洗尽的沧桑”那一章。由于当时没做笔记，现只大致回忆一下逻辑。由于北宋在与西夏和辽的对抗中，战争对货币的需求增加，国家货币铸造增加。北宋每年新增的货币量，从995年的每年铸铜钱80万贯，逐步增加到1000年前后的125万贯，1007年的183万贯，1045年的300万贯，到宋神宗元丰三年（1080年）达到了506万贯的顶峰！货币的增加，一方面，促进了北宋真宗和仁宗时期《清明上河图》的繁荣，另一方面，到北宋后期后期宋神宗时期，也造成了急遽的通货膨胀。通货的结果，一方面，货币流入到少部分官僚及当时的“银行家”手中，进而发展成地主，形成宋朝的土地兼并；另一方面，在四川等地由于货币贬值，金属货币携带不便，开始出现中国的最早的纸币——交子。宋神宗时期的王安石变法，目的就是为了改变这种财富不均、土地兼并、财政枯竭的积贫积弱现象，然而，却失败了！\n《吴晗说明史》《明朝那些事儿》《万历十五年》都是关于明史的记录，总体来说比较客观。虽然大部分历史学家对《万历十五年》推崇备至，几年前读过一次，这又看一次，却依然不是特别懂（之后再读）。倒觉得《吴晗说明史》和《明朝那些事》更白话。破土木堡之变，延续明朝历史近300年，“粉身碎骨浑不怕”两袖清风的于谦，顿时让自己又多了份正气，去到杭州，一定得去于谦墓瞻仰。也知道张居正是一个权臣，戚继光是贪官，而这些人的底子里却依然有为国为民的本性，人心的复杂岂是“好人坏人”两个词可以衡量！相较而言，“你是个好人，却并无用处“的海瑞就是一个异类。夏言的被迫害、严嵩的倒台、徐阶的隐忍奸诈、高拱专政、张居正的死后鞭尸，这一阶段变幻权利的斗争，对这些个人来说不过人世梦一场无不以提心掉胆的灾难结束，哪有老百姓平淡的向死而生来得舒坦。《明朝那些事》非常推荐一看，小说的品味却也不太失历史的严谨，最喜欢作者书后对读历史的观点，\n很多人问，为什么看历史，很多人回答，以史为鉴。 现在我来告诉你，以史为鉴，是不可能的。 因为我发现，其实历史没有变化，技术变了，衣服变了，饮食变了，这都是外壳，里面什么都没变化，还是几千年前那一套，转来转去，该犯的错误还是要犯，该杀的人还是要杀，岳飞会死，袁崇焕会死，再过一千年，还是会死。 所有发生的，是因为它有发生的理由，能超越历史的人，才叫以史为鉴，然而我们终究不能超越，因为我们自己的欲望和弱点。 所有的错误，我们都知道，然而终究改不掉。\n以史为镜也许可以，但以史为鉴就是一味饱含苦杏仁味的鸡汤，是一句屁话。所以也就不太在意能从历史中学到什么了。\n《月亮与六便士》这种颠覆我价值观念的书就像一击重锤敲醒了定势思维的脑袋。月亮与六便士讲的是：斯特里克兰德，他突然放弃银行工作，抛妻弃子，远走巴黎，在穷困潦倒时他的朋友施特略夫不顾妻子反对救助他。施特略夫是一个十足善良的人，\n有时候一个人的外貌同他的灵魂这么不相称，这实在是一件苦不堪言的事。施特略夫就是这样：他心里有罗密欧的热情，却生就一副托比·培尔契爵士的形体。他的禀性仁慈、慷慨，却不断闹出笑话来：他对美的东西从心眼里喜爱，但自己却只能创造出平庸的东西；他的感情非常细腻，但举止却很粗俗。他在处理别人的事务时很有手腕，但自己的事却弄得一团糟。大自然在创造这个人的时候，在他身上揉捏了这么多相互矛盾的特点，叫他面对着令他迷惑不解的冷酷人世，这是一个多么残忍的玩笑啊。\n斯特里克兰德没有感激施特略夫，他夺取了朋友妻子，而后又抛弃了她，最终她因他而自尽时，他无半点同情，\n“难道你不爱你的孩子们吗”？他说：“我对他们没有特殊感情”；\n“难道你连爱情都不需要吗？”他说：“爱情只会干扰我画画”。\n这样一个残酷、冷漠、自私、不尊重友情不尊重生命的人，最终却在他死后，他的画成为价值连城的艺术品。不尊重生活也可以有艺术，技术再好也未必表示他能有菩萨的心肠。斯特里克兰德就是这么一个人，\n他好象是一个终生跋涉的朝香者，永远思慕着一块圣地。盘踞在他心头的魔鬼对他毫无怜悯之情。世上有些人渴望寻获真理，他们的要求非常强烈，为了达到这个目的，就是叫他们把生活的基础完全打翻，也在所不惜。思特里克兰德就是这样一个人；只不过他追求的是美，而不是真理。\n我不禁想，自己到底欣赏的是斯特略夫的善良还是斯特里克兰德的为艺术而坚持的冷漠？我也不知道。\n相比于《月亮与六便士》中“不需要爱情”的斯特里克兰德，《了不起的盖茨比》中的盖茨比则是脱离现实的过分深情。这是一个很悲伤的故事：\n盖茨比年轻时与黛西热恋，因为没钱，黛西最后嫁给了富家子弟汤姆，而盖茨比到欧洲参加了第一次世界大战。战后归来，盖茨比通过非法的酒和食品买卖挣了大钱。他对黛西年年不忘，在黛西住处对面买了栋别墅。为吸引黛西的注意，盖茨比邀请明星好友，别墅里夜夜笙歌艳舞。后来，在尼克的安排下，盖茨比和已婚的黛西见了面，黛西的表情让盖茨比迷恋，盖茨比以为可以挽回昔日的恋情。汤姆也被邀请到别墅里，盖茨比、黛西、汤姆、尼克一起去纽约，盖茨比与黛西同一辆车，“于是在稍微凉快一点的暮色中向死亡驶去”。黛西因情绪激动开车撞死了汤姆的情妇玛特尔，肇事后逃走。玛特尔的丈夫威尔逊被汤姆挑拨，以为盖茨比是凶手，于是潜入盖茨比家中，杀死了盖茨比。威尔逊开枪自杀。事后，除了盖茨比的父亲，盖茨比昔日到别墅笙歌艳舞的好友唯恐避之不及，没有人来参加盖茨比的丧礼。黛西和汤姆携带行李逃去欧洲旅游。\n站在盖茨比的别墅，能看到黛西住处有一盏灯，那是盖茨比盲目崇拜黛茜的脱离现实的梦想，\n当我坐在那里缅怀那个古老的、未知的世界时，我也想到了盖茨比第一次认出了黛西的码头尽头的那盏绿灯时所感到的惊奇。他经历了漫长的道路才来到这片蓝色的草坪上，他的梦一定就像是近在眼前，他几乎不可能抓不住的。他不知道那个梦已经丢在他背后了，丢在这个城市那边那一片无垠的混饨之中不知什么地方了，那里合众国的黑黝黝的田野在夜色中向前伸展。 盖茨比信奉这盏绿灯，这个一年年在我们眼前渐渐远去的极乐的未来。它从前逃脱了我们的追求，不过那没关系――明天我们跑得更快一点，把胳臂伸得更远一点……总有一天…… 于是我们奋力向前划，逆流向上的小舟，不停地倒退，进入过去。\n盖茨比对爱情幻想付出生命的追求，而盖茨比死后人世的无情，狰狞的现实在可怕的噩梦之间辗转反侧，就是一段爵士般的挽歌！记起小说中尼克的话，\n盖茨比在我眼中有了生命，忽然之间从他那子宫般的毫无目的的豪华里分娩了出来。\n《撒哈拉的故事》是三毛和荷西刚结婚在撒哈拉沙漠定居的故事。入洞房，久病成医，偷看人洗澡，羊从房顶掉下来……每个小故事，仅在撒哈拉沙漠里平淡而真实。摘录故事中一些感动过我的标注，\n我在想，總有一天我們會死在這片荒原裡。\n我在想——也許——也許是我潛意識裡總有想結束自己生命的欲望。所以——病就來了。\n今天是回教開齋的節日，窗外碧空如洗，涼爽的微風正吹進來，夏日已經過去，沙漠美麗的秋天正在開始。\n生命的過程，無論是陽春白雪，青菜豆腐，我都得嘗嘗是什麼滋味，才不枉來走這麼一遭啊！\n撒哈拉沙漠是這麼的美麗，而這兒的生活卻是要付出無比的毅力來使自己適應下去啊！ 我沒有厭沙漠，我只是在習慣它的過程裡受到了小小的挫折。\n到此结束，北京的生活也是这么的美丽，而这儿的生活却要付出无比的毅力来使自己适应下去！理性地思考后发现，我选择离开北京，所谓的厌恶了它的霾、它的生活方式只不过是借口，我只是在习惯它的过程中受到了小小的挫折。\n","permalink":"http://localhost:1313/posts/%E6%AF%95%E4%B8%9A%E5%AD%A3%E8%AF%BB%E8%BF%87%E7%9A%84%E9%82%A3%E4%BA%9B%E4%B9%A6/","summary":"这些年我一直提醒自己一件事情，千万不要自己感动自己。大部分人看似的努力，不过是愚蠢导致的。什么熬夜看书到天亮，连续几天只睡几小时，多久没放假了，如果这些东西也值得夸耀，那么富士康流水线上任何一个人都比你努力多了。人难免天生有自怜的情绪，唯有时刻保持清醒，才能看清真正的价值在哪里。\n——于宙TEDx演讲：我们这一代人的困惑\n平时总拿追求安逸平凡作为生活的目标，其实是给自己懒惰找的借口而已。为什么要读书，下面是在知乎里看到的答案，\n我读过很多书，但后来大部分都被我忘记了，那阅读的意义是什么？\n答：当我还是个孩子的时候，我吃过很多食物，现在已经记不起来吃过什么了。但可以肯定的是，它们中的一部分已经长成为我的骨头和肉。\n2015年，因为经历，才深深体会了宫崎骏的一句话，\n你在的城市，天下起了雨，很想问你有没有带伞，可我却忍住了，因为我怕你说没带，而我却无能为力。\n为缓解这种忍耐的情绪，在下半年实习找工作之余，增加了一些读书量，如下图为2015年7月份以来的读书单。\n其中有一些是极力推荐的。我是个东坡迷，相较于林语堂的《苏东坡传》，感觉李一冰的更客观更详细——大部分以史学材料、东坡诗词和东坡同时代的文人札记（或书信等）为基础。东坡那“如食之有蝇，吐之乃已”的性格，即使在现代也是“一肚子的不合时宜”，估计也会沦落到晓松老师之流。然而，东坡身兼儒释道的正派而又不失率真豁达，才气逼人却是晓松老师不能比的。\n《目送》是台湾作家龙应台在老年时去回顾中年的感悟。在文中她写了生命中两件“小事”：儿子十六岁到美国当交换学生，在机场，她看着儿子通过护照检查、进入海关，背影倏地消失，没有回头；多年后，她父亲在医院的最后时光，她又看着轮椅上被护士推回房的父亲背影。通过这些事，她开始理解个人生命中最私密、最深埋、最不可言喻的“伤逝”和“舍”，并在文中写道：\n我慢慢地、慢慢地了解到，所谓父女母子一场，只不过意味着，你和他的缘分就是今生今世不断地在目送他的背影渐行渐远。\n虽还没到那个年纪，体悟不算太多，但也无法抗拒作者文字的朴素深沉。我记得这么一段描述，\n「火车突然停了，」母亲说，「车顶上趴着一堆人，有一个女的说憋不住了，无论如何要上厕所，就爬下来，她的小孩儿还留在车顶上头，让人家帮她抱一下。没想到，她一下来，车就动了。」\n小的时候，有一次和妈妈外出，也经历了同样的窘境。妈妈去上厕所还没回来，车就动了，那时候的我胆小，也不知道怎么和司机说，然后就一直害怕，“妈妈丢了，妈妈丢了…………”\n在我们整个成长的过程里，谁，教过我们怎么去面对痛苦、挫折、失败？它不在我们的家庭教育里，它不在小学、中学、大学的教科书或课程里，它更不在我们的大众传播里。家庭教育、学校教育、社会教育只教我们如何去追求卓越，从砍樱桃的华盛顿、悬梁刺股的孙敬、苏秦到平地起楼的比尔?盖茨，都是成功的典范。即使是谈到失败，目的只是要你绝地反攻，再度追求出人头地，譬如越王句践的卧薪尝胆，洗雪耻辱，譬如哪个战败的国王看见蜘蛛如何结网，不屈不挠。 我们拚命地学习如何成功冲刺一百米，但是没有人教过我们：你跌倒时，怎么跌得有尊严；你的膝盖破得血肉模糊时，怎么清洗伤口、怎么包扎；你痛得无法忍受时，用什么样的表情去面对别人；你一头栽下时，怎么治疗内心淌血的创痛，怎么获得心灵深层的平静，心像玻璃一样碎了一地时，怎么收拾？\n也成为一种教育悲哀的阐述——只教会如何坚持如何的努力，却没有告诉我们如何去缓解努力过程中带来的压力。\n《小王子》里记得有那么一段话居然影响到我写简历，\n如果你对大人们说：“我看到一幢玫瑰色的砖盖成的漂亮的房子，它的窗户上有天竺葵，屋顶上还有鸽子…”他们怎么也想象不出这种房子有多么好。必须对他们说：“我看见了一幢价值十万法郎的房子。”那么他们就会惊叫道：“多么漂亮的房子啊！”\n那就是，写简历的时候能用数字说话就用数字说话，别BB“啊我那个项目做得多好啊”你还不如直接说“我做的那个项目拉了100万投资”。我本来是寻找童真的，却收获到一点利益世俗。但是，却也有很多地方感动到心跳加速，比如小王子到玫瑰盛开的花园，拿玫瑰花园里的花和自己星球上的玫瑰做对比，\n“你们很美，但你们是空虚的。”小王子仍然在对她们说，“没有人能为你们去死。当然喽，我的那朵玫瑰花，一个普通的过路人以为她和你们一样。可是， 她单独一朵就比你们全体更重要，因为她是我浇灌的。因为她是我放在花罩中的。 因为她是我用屏风保护起来的。因为她身上的毛虫（除了留下两三只为了变蝴蝶 而外）是我除灭的。因为我倾听过她的怨艾和自诩，甚至有时我聆听着她的沉默。 因为她是我的玫瑰。”\n“因为她是我的玫瑰”，因为真心地认真地付出过，才会不一样，才有感动，才会感觉更美，然而却也会更难忘记。\n《他们最幸福》和《乖，摸摸头》是大冰以一背包客的身份，讲诉行走川藏过程中遇到的人和事，其中不乏感动。也许缺乏军人一般豪气的历练，“给你一碗酒，可以慰风尘”的越战老兵深深打动我；也许仅是屌丝一枚，椰子姑娘漂流记中“喂，这张床分我一半”的爱情结局令我羡慕。对长期浸淫在计算机领域的我，初读时，确实带来了很多的感动。然而，之后理性地细想，却发现在洒脱的文字底部透着一股子的矫情。也许故事是真的，却在行文中给原本的真实多了份刻意的渲染。所以大冰的书是那种适合缺乏感动的人初读的书，却不是沉浮世故的人耐读的书。\n《货币战争5》只是看了一部分，找到这部书市因为想找一些关于从经济角度解读宋史的材料，所以只读了“北宋兴亡，铅华洗尽的沧桑”那一章。由于当时没做笔记，现只大致回忆一下逻辑。由于北宋在与西夏和辽的对抗中，战争对货币的需求增加，国家货币铸造增加。北宋每年新增的货币量，从995年的每年铸铜钱80万贯，逐步增加到1000年前后的125万贯，1007年的183万贯，1045年的300万贯，到宋神宗元丰三年（1080年）达到了506万贯的顶峰！货币的增加，一方面，促进了北宋真宗和仁宗时期《清明上河图》的繁荣，另一方面，到北宋后期后期宋神宗时期，也造成了急遽的通货膨胀。通货的结果，一方面，货币流入到少部分官僚及当时的“银行家”手中，进而发展成地主，形成宋朝的土地兼并；另一方面，在四川等地由于货币贬值，金属货币携带不便，开始出现中国的最早的纸币——交子。宋神宗时期的王安石变法，目的就是为了改变这种财富不均、土地兼并、财政枯竭的积贫积弱现象，然而，却失败了！\n《吴晗说明史》《明朝那些事儿》《万历十五年》都是关于明史的记录，总体来说比较客观。虽然大部分历史学家对《万历十五年》推崇备至，几年前读过一次，这又看一次，却依然不是特别懂（之后再读）。倒觉得《吴晗说明史》和《明朝那些事》更白话。破土木堡之变，延续明朝历史近300年，“粉身碎骨浑不怕”两袖清风的于谦，顿时让自己又多了份正气，去到杭州，一定得去于谦墓瞻仰。也知道张居正是一个权臣，戚继光是贪官，而这些人的底子里却依然有为国为民的本性，人心的复杂岂是“好人坏人”两个词可以衡量！相较而言，“你是个好人，却并无用处“的海瑞就是一个异类。夏言的被迫害、严嵩的倒台、徐阶的隐忍奸诈、高拱专政、张居正的死后鞭尸，这一阶段变幻权利的斗争，对这些个人来说不过人世梦一场无不以提心掉胆的灾难结束，哪有老百姓平淡的向死而生来得舒坦。《明朝那些事》非常推荐一看，小说的品味却也不太失历史的严谨，最喜欢作者书后对读历史的观点，\n很多人问，为什么看历史，很多人回答，以史为鉴。 现在我来告诉你，以史为鉴，是不可能的。 因为我发现，其实历史没有变化，技术变了，衣服变了，饮食变了，这都是外壳，里面什么都没变化，还是几千年前那一套，转来转去，该犯的错误还是要犯，该杀的人还是要杀，岳飞会死，袁崇焕会死，再过一千年，还是会死。 所有发生的，是因为它有发生的理由，能超越历史的人，才叫以史为鉴，然而我们终究不能超越，因为我们自己的欲望和弱点。 所有的错误，我们都知道，然而终究改不掉。\n以史为镜也许可以，但以史为鉴就是一味饱含苦杏仁味的鸡汤，是一句屁话。所以也就不太在意能从历史中学到什么了。\n《月亮与六便士》这种颠覆我价值观念的书就像一击重锤敲醒了定势思维的脑袋。月亮与六便士讲的是：斯特里克兰德，他突然放弃银行工作，抛妻弃子，远走巴黎，在穷困潦倒时他的朋友施特略夫不顾妻子反对救助他。施特略夫是一个十足善良的人，\n有时候一个人的外貌同他的灵魂这么不相称，这实在是一件苦不堪言的事。施特略夫就是这样：他心里有罗密欧的热情，却生就一副托比·培尔契爵士的形体。他的禀性仁慈、慷慨，却不断闹出笑话来：他对美的东西从心眼里喜爱，但自己却只能创造出平庸的东西；他的感情非常细腻，但举止却很粗俗。他在处理别人的事务时很有手腕，但自己的事却弄得一团糟。大自然在创造这个人的时候，在他身上揉捏了这么多相互矛盾的特点，叫他面对着令他迷惑不解的冷酷人世，这是一个多么残忍的玩笑啊。\n斯特里克兰德没有感激施特略夫，他夺取了朋友妻子，而后又抛弃了她，最终她因他而自尽时，他无半点同情，\n“难道你不爱你的孩子们吗”？他说：“我对他们没有特殊感情”；\n“难道你连爱情都不需要吗？”他说：“爱情只会干扰我画画”。\n这样一个残酷、冷漠、自私、不尊重友情不尊重生命的人，最终却在他死后，他的画成为价值连城的艺术品。不尊重生活也可以有艺术，技术再好也未必表示他能有菩萨的心肠。斯特里克兰德就是这么一个人，\n他好象是一个终生跋涉的朝香者，永远思慕着一块圣地。盘踞在他心头的魔鬼对他毫无怜悯之情。世上有些人渴望寻获真理，他们的要求非常强烈，为了达到这个目的，就是叫他们把生活的基础完全打翻，也在所不惜。思特里克兰德就是这样一个人；只不过他追求的是美，而不是真理。\n我不禁想，自己到底欣赏的是斯特略夫的善良还是斯特里克兰德的为艺术而坚持的冷漠？我也不知道。\n相比于《月亮与六便士》中“不需要爱情”的斯特里克兰德，《了不起的盖茨比》中的盖茨比则是脱离现实的过分深情。这是一个很悲伤的故事：\n盖茨比年轻时与黛西热恋，因为没钱，黛西最后嫁给了富家子弟汤姆，而盖茨比到欧洲参加了第一次世界大战。战后归来，盖茨比通过非法的酒和食品买卖挣了大钱。他对黛西年年不忘，在黛西住处对面买了栋别墅。为吸引黛西的注意，盖茨比邀请明星好友，别墅里夜夜笙歌艳舞。后来，在尼克的安排下，盖茨比和已婚的黛西见了面，黛西的表情让盖茨比迷恋，盖茨比以为可以挽回昔日的恋情。汤姆也被邀请到别墅里，盖茨比、黛西、汤姆、尼克一起去纽约，盖茨比与黛西同一辆车，“于是在稍微凉快一点的暮色中向死亡驶去”。黛西因情绪激动开车撞死了汤姆的情妇玛特尔，肇事后逃走。玛特尔的丈夫威尔逊被汤姆挑拨，以为盖茨比是凶手，于是潜入盖茨比家中，杀死了盖茨比。威尔逊开枪自杀。事后，除了盖茨比的父亲，盖茨比昔日到别墅笙歌艳舞的好友唯恐避之不及，没有人来参加盖茨比的丧礼。黛西和汤姆携带行李逃去欧洲旅游。\n站在盖茨比的别墅，能看到黛西住处有一盏灯，那是盖茨比盲目崇拜黛茜的脱离现实的梦想，\n当我坐在那里缅怀那个古老的、未知的世界时，我也想到了盖茨比第一次认出了黛西的码头尽头的那盏绿灯时所感到的惊奇。他经历了漫长的道路才来到这片蓝色的草坪上，他的梦一定就像是近在眼前，他几乎不可能抓不住的。他不知道那个梦已经丢在他背后了，丢在这个城市那边那一片无垠的混饨之中不知什么地方了，那里合众国的黑黝黝的田野在夜色中向前伸展。 盖茨比信奉这盏绿灯，这个一年年在我们眼前渐渐远去的极乐的未来。它从前逃脱了我们的追求，不过那没关系――明天我们跑得更快一点，把胳臂伸得更远一点……总有一天…… 于是我们奋力向前划，逆流向上的小舟，不停地倒退，进入过去。\n盖茨比对爱情幻想付出生命的追求，而盖茨比死后人世的无情，狰狞的现实在可怕的噩梦之间辗转反侧，就是一段爵士般的挽歌！记起小说中尼克的话，\n盖茨比在我眼中有了生命，忽然之间从他那子宫般的毫无目的的豪华里分娩了出来。\n《撒哈拉的故事》是三毛和荷西刚结婚在撒哈拉沙漠定居的故事。入洞房，久病成医，偷看人洗澡，羊从房顶掉下来……每个小故事，仅在撒哈拉沙漠里平淡而真实。摘录故事中一些感动过我的标注，\n我在想，總有一天我們會死在這片荒原裡。\n我在想——也許——也許是我潛意識裡總有想結束自己生命的欲望。所以——病就來了。\n今天是回教開齋的節日，窗外碧空如洗，涼爽的微風正吹進來，夏日已經過去，沙漠美麗的秋天正在開始。\n生命的過程，無論是陽春白雪，青菜豆腐，我都得嘗嘗是什麼滋味，才不枉來走這麼一遭啊！\n撒哈拉沙漠是這麼的美麗，而這兒的生活卻是要付出無比的毅力來使自己適應下去啊！ 我沒有厭沙漠，我只是在習慣它的過程裡受到了小小的挫折。\n到此结束，北京的生活也是这么的美丽，而这儿的生活却要付出无比的毅力来使自己适应下去！理性地思考后发现，我选择离开北京，所谓的厌恶了它的霾、它的生活方式只不过是借口，我只是在习惯它的过程中受到了小小的挫折。","title":"毕业季读过的那些书"}]